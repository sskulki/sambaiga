{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Variational Inference (SVI)\n",
    "> This post introduce  stochastic gradient based algorithm (SVI) used in practise to do variational inference under mean filed assumptions. It also present two important tricks namely re-parametrization trick and amortized inference that are useful when using SVI in solving problems.\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [jupyter]\n",
    "- image: images/svi.jpg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [previous post]()  introduced the basic principle of Variational Inference (VI) as one of the approach used to approximate difficult probability distribution, derived the ELBO function and discussed about Mean Field Variational Inference (MFVI) and the Coordinate Ascent Variational Inference (CAVI) algorithms. This post introduce another  stochastic gradient based algorithm (SVI) used in practise to do VI under mean filed assumptions. It also present two important tricks re-parametrization trick and amortized inference that are useful when using SVI in solving problems.\n",
    "\n",
    "## Stochastic Variational Inference (SVI)\n",
    "Consider the graphical model of the observations $\\mathbf{x}$ and latent variable $\\mathbf{z}=\\{\\theta, z\\}$ in figure 1 where $\\theta$ is the global variable and $z = \\{z_1, \\ldots z_n\\}$  is the local (per-data-point) variable such that:\n",
    "\n",
    "![](my_icons/VI.png)\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x},\\mathbf{z}) = p(\\theta|\\alpha)\\prod_{i=1}^N p(x_i|z_i, \\theta)\\cdot p(z_i|\\alpha)\n",
    "$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly the variational parameters are given by $\\lambda = \\{\\gamma, \\phi\\} $ where the variational parameter $\\gamma$ correspond to latent variable  and  $\\phi$ denote set of local variational parameters. The variational distribution $q(\\mathbf{z}\\mid \\phi)$ is given by\n",
    "\n",
    "$$\n",
    "q(\\mathbf{z}\\mid \\phi) = q(\\theta|\\gamma)\\prod_{i=1}^N q(z_i|\\phi_i, \\alpha)\n",
    "$$\n",
    "\n",
    "which  also depend on hyper-parameter $\\alpha$. The ELBO of this graphical model $\\mathcal{L}_{VI}(q) = \\mathbb{E}_q[ \\log p(\\mathbf{x},\\mathbf{z}, \\alpha) -\\log q(\\mathbf{z}, \\gamma)]$ has the following form:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\mathcal{L}_{VI}(q) &= \\mathbb{E}_q[ \\log p(\\theta|\\alpha)- \\log q(\\theta|\\gamma)] \\\\\n",
    "&+ \\sum_{i=1}^{N}\\mathbb{E}_q[\\log p(z_i|\\theta) \n",
    "+ \\log p(x_i|z_i, \\theta)-\\log q(z_i|\\phi_i)]\n",
    "\\end{split}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation above could be optimized by CAVI algorithm discussed in previous post which is expensive for large data sets. The CAVI algorithm scales with $N$ as it require to optimize the local variational parameters for each data point before re-estimating the global variational parameters. \n",
    "\n",
    "Unlike CAI, SVI uses stochastic optimization to fit the global variational parameters by repeatedly sub-sample the data to form stochastic estimate of ELBO. In every iteration one randomly selects mini-batches of size $b_{sz}$  to obtain a stochastic estimate of ELBO.\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\mathcal{L}_{VI}(q) &= \\mathbb{E}_q[ \\log p(\\theta|\\alpha)- \\log q(\\theta|\\gamma)] \\\\\n",
    "&+ \\frac{N}{b_{sz}}\\sum_{s=1}^{b_{sz}}\\mathbb{E}_q[\\log p(z_{i_s}|\\theta) + \\log p(x_{i_s}|z_{i_s}, \\theta)-\\log q(z_{i_s}|\\phi_{i_s})]\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "SVI algorithms follow noisy estimates of the gradient with a decreasing step size which is often  cheaper to compute than the true gradient. Following such noisy estimates allows SVI to escape  shallow local optima of complex objective functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Gradient for SVI\n",
    "\n",
    "To solve the optimization problem standard gradient-based methods such as SGD, Adam or Adagrad can be used. However, for SVI these gradient based methods cause slow convergence or converge to inferior local models. This is because, gradient based methods use the following update  \n",
    "\n",
    "$$\n",
    "\\theta^{t+1}=\\theta^t + \\alpha \\frac{\\partial \\mathcal{L}_{VI}(q)}{\\partial \\theta} \n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}_{VI}(q)}{\\partial \\theta} =\\frac{\\partial \\mathcal{L}_{VI}(q)}{\\partial \\theta_1}, \\ldots \\frac{\\partial \\mathcal{L}_{VI}(q)}{\\partial \\theta_k}\n",
    "$$ \n",
    "\n",
    "is the the gradient vector which point in the direction where the function increases most quickly while the changes in the function are measured with respect to euclidean distance. As the result, if the euclidean distance between the variational parameter being optimized is not good measure of variation in objective function then gradient descent will move suboptimal through the parameter value. \n",
    "\n",
    "Consider the following  two set of gausian distributions $$\\{d_{(1)}=\\mathcal{N}(-2, 3), d_{(2)}=\\mathcal{N}(2, 3)\\}$$ and $$\\{d_{(1)}=\\mathcal{N}(-2, 1), d_{(2)}=\\mathcal{N}(2, 1)\\}$$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The euclidean distance between the two distributions $d_{}=\\sqrt{(\\mu_1-\\mu_2)^2+ (\\sigma^2_1-\\sigma^2_2)^2}=4$ It clear that, considering only the euclidean distance the two images are the same. However, when we consider the shape of the distribution, the distance is different in the first and second image. In the first image, the KL-divergence should be lower as there is more overlap between between the two distribution unlike the second image where their support barely overlap. The reason for this difference is that probability distribution do not naturally fit in euclidean space rather it fit on a statistical manifold also called [Riemannian manifold](https://en.wikipedia.org/wiki/Riemannian_manifold). \n",
    "\n",
    "Statistical manifold give a natural way of measuring distances between distribution that euclidean distance use in SGD. A common Riemannian metric for statistical manifold is the [fisher information matrix](https://wiseodd.github.io/techblog/2018/03/11/fisher-information/) defined by \n",
    "\n",
    "$$\n",
    "F_{\\lambda} = \\mathbb{E}_{p(x;\\lambda)}[\\nabla \\log p(x;\\lambda) (\\nabla \\log p(x;\\theta))^T ]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be showed that the fisher information matrix $F_{\\lambda}$ is  the second derivative of the KL divergence between two distributions.\n",
    "\n",
    "$$\n",
    "F_{\\theta} = \\nabla^2_{\\theta} KL(q(x;\\lambda)||p(x;\\theta))\n",
    "$$\n",
    "\n",
    "Thus for SVI, the standard gradients descent techniques can be replaced  with the natural gradient as follows:\n",
    "\n",
    "$$\n",
    "        \\tilde{\\nabla_{q}} \\mathcal{L}(q) = F^{-1} \\nabla{q}\\mathcal{L}_{VI}(q)\n",
    "$$\n",
    "\n",
    "The update procedure for natural gradient can be summarized as follows:\n",
    "\n",
    "1. Compute the loss $\\mathcal{L}_{VI}(q)$\n",
    "2. Compute the gradient of the loss $\\nabla{q}\\mathcal{L}_{VI}(q)$\n",
    "3. Compute the Fisher Information Matrix F.\n",
    "4. Compute the natural gradient $\\tilde{\\nabla_{q}} \\mathcal{L}_{VI}(q)$\n",
    "5. Update the parameter $q^{t+1} =q^t - \\alpha \\tilde{\\nabla_{\\theta}}\\mathcal{L}_{VI}(q)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using natural gradient instead of standard gradients simplify SVI gradient update. However the same conditions for convergence as standard SDG have to be fulfilled. First, the mini-batch indices must be drawn uniformly at random size where the size $b_{sz}$ of the mini-batch must satisfies $1\\leq b_{sz} \\leq N$ The learning rate $\\alpha$ needs to decrease with iterations $t$ satisying the [Robbins Monro conditions](https://en.wikipedia.org/wiki/Stochastic_approximation) $\\sum_{t=1}^{\\infty} \\alpha_t =\\infty$ and $\\sum_{t=1}^{\\infty} \\alpha_t^2 <\\infty$ This guarantee that every point in the parameter space can be reached while the gradient noise decreases quickly enough to ensure convergence.\n",
    "\n",
    "The next section presents two important tricks namely re-parametrization trick and amortized inference that are useful when using SVI in solving problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-parametrization trick\n",
    "\n",
    "Consider the graphical model presented in figure 1, where gradient based stochastic optimization is used to learn the variational parameter $\\phi$. For example; for Gaussian distribution  $q_{\\phi}(z|x)=\\mathcal{N}(\\mu_{\\phi}(x), \\Sigma_{\\phi}(x))$\n",
    "\n",
    "To maximize the likelihood of the data, we need to back propagate the loss to the parameter $\\phi$ across the distribution of $z$ or across sample $z\\sim q_\\phi(z \\mid x) $ However, it is difficulty to back-propagate through random variable. To address this problem, the re-parametrization trick is used.\n",
    "\n",
    "First let consider the [Law of the Unconscious Statistician (LOTUS)](https://en.wikipedia.org/wiki/Law_of_the_unconscious_statistician), that is used to calculate the expected value of a function $g(\\epsilon)$ of a random variable $\\epsilon$ when only the probability distribution $p(\\epsilon)$ of $\\epsilon$ is known. The Law state that: \n",
    ">To compute the expectation of a measurable function $g(.)$ of a random variable $\\epsilon$, we have to integrate $g(\\epsilon)$ with respect to the distribution function of $\\epsilon$, that is:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(g(\\epsilon)) = \\int g(\\epsilon)dF_{\\epsilon}(\\epsilon)\n",
    "$$\n",
    "\n",
    "In other words, to compute the expectation of $z =g(\\epsilon)$ we only need to know $g(.)$ and the distribution of $\\epsilon$. We do not need to explicitly know the distribution of $z$. Thus the above equation can be expression in the convenient alternative notation:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{\\epsilon \\sim p(\\epsilon)}(g(\\epsilon)) = \\mathbb{E}_{z \\sim p(z)} (z)\n",
    "$$\n",
    "\n",
    "Therefore the reparameteriztaion trick states that:  \n",
    ">A random variable $z$ with distribution $q_{\\phi}(z, \\phi)$ which is independent to $\\phi$ can be expressed as transformation of random variable $\\epsilon \\sim p(\\epsilon)$ that come from noise distribution such as uniform or gaussian such that $z = g(\\phi, \\epsilon)$\n",
    "\n",
    "For instance for Gaussian variable $z$ in the above example \n",
    "\n",
    "$$\n",
    "z = \\mu(\\phi) + \\sigma^2(\\phi)\\cdot \\epsilon\n",
    "$$\n",
    "\n",
    "where $\\epsilon \\sim \\mathcal{N}(0, 1)$. Since $p(\\epsilon)$ is independent of the parameter of $q_{\\phi}(z, \\phi)$, we can apply the change of variables in integral theory to compute any expectation over $z$ or any expectation over  $\\phi$. The SDG estimator can therefore be estimated by pulling the gradient into expectations and approximating it by samples from the noise \n",
    "distribution such that  for any measurable function $f_{\\theta}(.)$:\n",
    "\n",
    "$$\n",
    "\\Delta_{\\phi}\\mathbb{E}_{z\\sim p_{\\phi}(z)} = \\frac{1}{M}\\sum_{i=1}^M \\Delta f(g(\\phi, \\epsilon_i)) \n",
    "$$\n",
    "\n",
    "where $\\epsilon_i\\sim p(\\epsilon)$ , $f_{\\theta}(.)$ must be differentiable w.r.t  its input $z$ and $g(\\phi, \\epsilon_i)$ must exist and be differentiable with respect to $\\phi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amortized Variational Inference\n",
    "\n",
    "Consider the graphical model presented in figure 1 where ecah data point $x_i$ is governed by its latent variable $z_i$ with variational parameter $phi_i$ such that \n",
    "\n",
    "$$\n",
    "q(\\mathbf{z}\\mid \\phi) = q(\\theta|\\gamma)\\prod_{i=1}^N q(z_i|\\phi_i, \\alpha)\n",
    "$$\n",
    "\n",
    "Using traditional SVI make it necessary to optimize $\\phi_i$ for each data point $x_i$. As the results the number parameters to be optimized will grows with the number of observations $x$. This is not ideal for larger datasets. Apart from that, it requires one to re-run the optimization procedure in case of new observation or when we have to perform inference. To address these problem amortized VI introduce a parametrized function that maps from observation space to the parameter of the approximate posterior distribution. \n",
    "\n",
    "Amortized VI try to learn from past inference/pre-computation so that future inferences run faster. Instead of approximating separate variables for each data point $x_i$, amortized VI assume that the local variational parameter $\\phi$ can be predicted by a parametrized function $f_{\\phi}(.)$ of data whose parameters are shared across all data points. Thus instead of introducing local variational parameter, we learn a single parametric function and work with a variational distribution that has the form \n",
    "\n",
    "$$\n",
    "q(\\mathbf{z}\\mid \\phi) = q(\\theta|\\gamma)\\prod_{i=1}^N q(z_i|f_{\\phi}(.))\n",
    "$$\n",
    "\n",
    "where $f_{\\phi}(.)$ is the deep neural net function of $z$\n",
    "\n",
    "\n",
    "Deep neural network used in this context are called [inference networks](). Therefore amortized inference with inference networks combines probabilistic modelling with representation power of deep learning. Using amortized VI instead of traditional VI, has two important advantages. First the number of variational parameters remain constant with respect to the data size. We only need to specify the parameter of the neural networks which is independent to the number of observations. Second, for new observation or during inference all we need to do is to call the inference network. As the result, we can invest time upfront optimizing the inference network and during inference we use the trained network for fast inference.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a binary regression problem where we are intrested on predicting whether or not a customer will subscribe a term deposit after the marketing campaign the bank performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.optim as optim\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pyro.infer import Trace_ELBO, SVI\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, LabelBinarizer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_binarize(df):\n",
    "    bin_enc = LabelBinarizer()\n",
    "    return bin_enc.fit_transform(df)\n",
    "\n",
    "def standardize(df):\n",
    "    std_enc = StandardScaler()\n",
    "    return std_enc.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/bank.csv\")\n",
    "\n",
    "## Encode target variable\n",
    "LE=LabelEncoder()\n",
    "target=LE.fit_transform(data.deposit.values)\n",
    "features = data.drop(columns=\"deposit\")\n",
    "\n",
    "# Separate both dataframes into \n",
    "numeric_data = data.select_dtypes(exclude=\"object\")\n",
    "categ_data  = data.drop(columns=numeric_data)\n",
    "#categ_data = pd.get_dummies(categ_data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in categ_data.columns:\n",
    "    categ_data[column] = label_binarize(categ_data[column].values)\n",
    "    \n",
    "columns_int = [\"campaign\", \"pdays\", \"previous\"]\n",
    "for i, column  in enumerate(columns_int):\n",
    "    numeric_data[column] = label_binarize(numeric_data[column].values)\n",
    "    \n",
    "    \n",
    "numeric_column = [\"age\", \"balance\", \"day\", \"duration\"]\n",
    "for i, column  in enumerate(numeric_column):\n",
    "    numeric_data[column] = standardize(numeric_data[[column]].values)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.concat([numeric_data, categ_data], 1)\n",
    "features = torch.tensor(features.values).float()\n",
    "target   = torch.tensor(target).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=10, test_size=0.15, random_state=0)\n",
    "train, test = next(sss.split(features.numpy(), target.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = features[train], features[test]\n",
    "y_train, y_test = target[train], target[test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.nn import PyroSample\n",
    "from torch import nn\n",
    "from pyro.nn import PyroModule\n",
    "assert issubclass(PyroModule[nn.Linear], nn.Linear)\n",
    "assert issubclass(PyroModule[nn.Linear], PyroModule)\n",
    "\n",
    "class BayesianRegression(PyroModule):\n",
    "    def __init__(self, in_features, out_features=1):\n",
    "        super().__init__()\n",
    "        self.linear = PyroModule[nn.Linear](in_features, out_features)\n",
    "        self.linear.weight = PyroSample(dist.Normal(0., 1.).expand([out_features, in_features]).to_event(2))\n",
    "        self.linear.bias = PyroSample(dist.Normal(0., 10.).expand([out_features]).to_event(1))\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        sigma = pyro.sample(\"sigma\", dist.Uniform(0., 10.))\n",
    "        mean = torch.sigmoid(self.linear(x).squeeze(-1))\n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            obs = pyro.sample(\"obs\", dist.Bernoulli(mean), obs=y)\n",
    "        return mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer.autoguide import AutoDiagonalNormal\n",
    "\n",
    "model = BayesianRegression(features.size(-1), 1)\n",
    "guide = AutoDiagonalNormal(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer import SVI, Trace_ELBO\n",
    "\n",
    "\n",
    "adam = pyro.optim.Adam({\"lr\": 0.03})\n",
    "svi = SVI(model, guide, adam, loss=Trace_ELBO())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 0001] loss: 0.0208\n",
      "[iteration 0101] loss: 0.0188\n",
      "[iteration 0201] loss: 0.0167\n",
      "[iteration 0301] loss: 0.0160\n",
      "[iteration 0401] loss: 0.0151\n",
      "[iteration 0501] loss: 0.0146\n",
      "[iteration 0601] loss: 0.0133\n",
      "[iteration 0701] loss: 0.0136\n",
      "[iteration 0801] loss: 0.0135\n",
      "[iteration 0901] loss: 0.0135\n",
      "[iteration 1001] loss: 0.0129\n",
      "[iteration 1101] loss: 0.0137\n",
      "[iteration 1201] loss: 0.0124\n",
      "[iteration 1301] loss: 0.0123\n",
      "[iteration 1401] loss: 0.0125\n",
      "[iteration 1501] loss: 0.0132\n",
      "[iteration 1601] loss: 0.0122\n",
      "[iteration 1701] loss: 0.0120\n",
      "[iteration 1801] loss: 0.0120\n",
      "[iteration 1901] loss: 0.0131\n",
      "[iteration 2001] loss: 0.0121\n",
      "[iteration 2101] loss: 0.0120\n",
      "[iteration 2201] loss: 0.0119\n",
      "[iteration 2301] loss: 0.0118\n",
      "[iteration 2401] loss: 0.0118\n",
      "[iteration 2501] loss: 0.0118\n",
      "[iteration 2601] loss: 0.0114\n",
      "[iteration 2701] loss: 0.0118\n",
      "[iteration 2801] loss: 0.0117\n",
      "[iteration 2901] loss: 0.0119\n",
      "[iteration 3001] loss: 0.0122\n",
      "[iteration 3101] loss: 0.0119\n",
      "[iteration 3201] loss: 0.0118\n",
      "[iteration 3301] loss: 0.0122\n",
      "[iteration 3401] loss: 0.0121\n",
      "[iteration 3501] loss: 0.0117\n",
      "[iteration 3601] loss: 0.0119\n",
      "[iteration 3701] loss: 0.0118\n",
      "[iteration 3801] loss: 0.0120\n",
      "[iteration 3901] loss: 0.0117\n",
      "[iteration 4001] loss: 0.0119\n",
      "[iteration 4101] loss: 0.0129\n",
      "[iteration 4201] loss: 0.0117\n",
      "[iteration 4301] loss: 0.0118\n",
      "[iteration 4401] loss: 0.0117\n",
      "[iteration 4501] loss: 0.0119\n",
      "[iteration 4601] loss: 0.0118\n",
      "[iteration 4701] loss: 0.0122\n",
      "[iteration 4801] loss: 0.0122\n",
      "[iteration 4901] loss: 0.0120\n"
     ]
    }
   ],
   "source": [
    "for j in range(5000):\n",
    "    # calculate the loss and take a gradient step\n",
    "    loss = svi.step(X_train, y_train.float())\n",
    "    if j % 100 == 0:\n",
    "        print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss / len(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_loc tensor([-0.0071,  0.0229, -0.0480,  0.8757,  0.0749, -0.9454, -0.9454,  0.0240,\n",
      "        -0.1021, -0.3675, -0.1923, -0.9378, -0.4416, -0.0203,  0.2147, -1.3860,\n",
      "         6.3369], requires_grad=True)\n",
      "w_scale tensor([-1.9725, -2.2263, -1.8859, -1.4443, -1.4062, -1.3978, -1.3952, -1.3166,\n",
      "        -1.2706, -1.2814, -0.7051, -1.3490, -1.2376, -1.4575, -1.2055, -1.3631,\n",
      "        -0.8033], requires_grad=True)\n",
      "b_loc tensor(-0.9499, requires_grad=True)\n",
      "b_scale tensor(-1.4293, requires_grad=True)\n",
      "sigma_loc tensor(0.4712, requires_grad=True)\n",
      "sigma_scale tensor(1.0128, requires_grad=True)\n",
      "AutoDiagonalNormal.loc Parameter containing:\n",
      "tensor([ 2.0622e-01, -3.7485e-03, -3.0043e-02, -8.0583e-02,  6.4904e-01,\n",
      "         1.8374e-01, -3.6323e-01, -3.5512e-01, -2.5773e-02,  5.6965e-02,\n",
      "        -1.7738e-01,  4.0707e-02, -5.3365e-01, -2.6170e-01,  3.2216e-01,\n",
      "         4.4906e-02, -2.5154e-01,  1.2037e+01, -5.6873e+00])\n",
      "AutoDiagonalNormal.scale tensor([1.8662, 0.1736, 0.1103, 0.1989, 0.2169, 0.2756, 0.2084, 0.2840, 0.4834,\n",
      "        0.5076, 0.3287, 0.8293, 0.2835, 0.3674, 0.2236, 0.5573, 0.4754, 0.2786,\n",
      "        0.2006])\n"
     ]
    }
   ],
   "source": [
    "guide.requires_grad_(False)\n",
    "\n",
    "for name, value in pyro.get_param_store().items():\n",
    "    print(name, pyro.param(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sigma': [tensor(2.5875), tensor(5.5137), tensor(8.1229)],\n",
       " 'linear.weight': [tensor([[-1.2086e-01, -1.0444e-01, -2.1471e-01,  5.0276e-01, -2.1774e-03,\n",
       "           -5.0379e-01, -5.4666e-01, -3.5181e-01, -2.8543e-01, -3.9910e-01,\n",
       "           -5.1865e-01, -7.2488e-01, -5.0951e-01,  1.7136e-01, -3.3100e-01,\n",
       "           -5.7217e-01,  1.1849e+01]]),\n",
       "  tensor([[-3.7485e-03, -3.0043e-02, -8.0583e-02,  6.4904e-01,  1.8374e-01,\n",
       "           -3.6323e-01, -3.5512e-01, -2.5773e-02,  5.6965e-02, -1.7738e-01,\n",
       "            4.0707e-02, -5.3365e-01, -2.6170e-01,  3.2216e-01,  4.4906e-02,\n",
       "           -2.5154e-01,  1.2037e+01]]),\n",
       "  tensor([[ 0.1134,  0.0444,  0.0535,  0.7953,  0.3697, -0.2227, -0.1636,  0.3003,\n",
       "            0.3994,  0.0443,  0.6001, -0.3424, -0.0139,  0.4730,  0.4208,  0.0691,\n",
       "           12.2247]])],\n",
       " 'linear.bias': [tensor([-5.8226]), tensor([-5.6873]), tensor([-5.5520])]}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guide.quantiles([0.25, 0.5, 0.75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer import Predictive\n",
    "\n",
    "\n",
    "def summary(samples):\n",
    "    site_stats = {}\n",
    "    for k, v in samples.items():\n",
    "        site_stats[k] = {\n",
    "            \"mean\": torch.mean(v, 0),\n",
    "            \"std\": torch.std(v, 0),\n",
    "            \"5%\": v.kthvalue(int(len(v) * 0.05), dim=0)[0],\n",
    "            \"95%\": v.kthvalue(int(len(v) * 0.95), dim=0)[0],\n",
    "        }\n",
    "    return site_stats\n",
    "\n",
    "predictive = Predictive(model, guide=guide, num_samples=800,\n",
    "                        return_sites=(\"linear.weight\", \"obs\", \"_RETURN\"))\n",
    "samples = predictive(X_test)\n",
    "pred_summary = summary(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = pred_summary[\"_RETURN\"]\n",
    "y = pred_summary[\"obs\"]\n",
    "predictions = pd.DataFrame({\n",
    "    \"age\": X_test[:, 0],\n",
    "    \"rugged\": X_test[:, 1],\n",
    "    \"mu_mean\": mu[\"mean\"],\n",
    "    \"mu_perc_5\": mu[\"5%\"],\n",
    "    \"mu_perc_95\": mu[\"95%\"],\n",
    "    \"y_mean\": y[\"mean\"],\n",
    "    \"y_perc_5\": y[\"5%\"],\n",
    "    \"y_perc_95\": y[\"95%\"],\n",
    "    \"true_gdp\": y_test,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.9962, 0.0012,  ..., 0.9987, 0.9950, 0.9975])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a28bc04d0>]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(y['mean'], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reference\n",
    "\n",
    "1. [[Cheng Zhang,(2017)]](https://arxiv.org/abs/1711.05597):\n",
    "Advances in Variational Inference.\n",
    "2. [[Daniel Ritchie,(2016)]](https://arxiv.org/abs/1610.05735):Deep Amortized Inference for Probabilistic Programs.\n",
    "3. [[Andrew Miller,(2016)]](https://arxiv.org/abs/1610.05735):Natural Gradients and Stochastic Variational Inference.\n",
    "4.  [Shakir Mohamed](https://www.shakirm.com/papers/VITutorial.pdf):Variational Inference  for Machine Learning. \n",
    "5.  [DS3 workshop](https://emtiyaz.github.io/teaching/ds3_2018/ds3.html):Approximate Bayesian Inference: Old and New.\n",
    "6.  [Variational Inference and Deep Generative Models](https://github.com/philschulz/VITutorial):Variational Inference for NLP audiences\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
