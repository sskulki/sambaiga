---
layout: post
title:  The Basic of Hidden Markov Model
thumbnail: img/post/hmmpost.jpg
summary: >
  This post review basic of HMM and its implementation in Python.

mathjax: true 
---

## Introduction

HMM is a Markov model whose states are not directly observed instead each state is characterised by a probability distribution function modelling the observation corresponding to that state. HMM has been extensively used in temporal pattern recognition such as speech, handwriting, gesture recognition, robotics, biological sequences and recently in energy disaggregation. This tutorial will introduce the basic concept of HMM.

There are two variables in HMM: observed variables and hidden variables where the sequences of hidden variables forms a Markov process as shown in figure below. In the context of NILM, the hidden variables are used to model states(ON,OFF, standby etc) of individual appliances and the observed variables are used to model the electric usage. HMMs has been widely used in most of the recently proposed NILM approach because it represents well the individual appliance internal states which are not directly observed in the targeted energy consumption.
{% include image.html src="/assets/img/post/hmm.png"
                      caption="HMM graphical model" 
                      desc=""
                      %}



A typical HMM is characterised by the following: 

- The finite set of hidden states  $$S $$ (e.g ON, stand-by, OFF, etc.) of an appliance,  $$S = \{s_1, s_2....,s_N\} $$. 
- The finite set of  $$M $$ observable symbol  $$Y $$ per states (power consumption) observed in each state,  $$Y = \{y_1, y_2....,y_M\} $$. The observable symbol  $$Y $$ can be discrete or a continuous set. 
- The transition matrix   $$ \mathbf{A}=\{a_{ij},1\leq i,j \geq N\}  $$ represents the probability of moving from state  $$s_{t-1}=i $$ to  $$s_t =j $$ such that:  $$a_{ij} = P(s_{t} =j \mid s_{t-1}=i) $$, with  $$a_{ij} \leq 0 $$ and where  $$s_t $$ denotes the state occupied by the system at time  $$t $$. The matrix  $$\mathbf{A} $$ is  $$ N x N $$.
- The emission matrix  $$\mathbf{B} =\{b_j(k)\}  $$ representing the probability of emission of symbol  $$k $$  $$\epsilon $$  $$ Y $$ when system state is  $$s_t=j $$ such that:  $$b_j(k) = p(y_t = k  \mid  s_t=j)   $$ The matrix  $$\mathbf{B} $$ is an  $$N x M  $$. The emission probability can be discrete or continous distribution. If the emission is descrete a multinomial distribution is used and multivariate Gaussian distribution is usually used for continous emission.
- And the initial state probability distribution  $$\mathbf{\pi}  = \{\pi_i \} $$ indicating the probability of each state of the hidden variable  at  $$t = 1 $$ such that,  $$\pi _i = P(q_1 = s_i), 1 \leq i \geq N $$.


The complete HMM specification requires;

1. Finite set of hidden states $$N $$ and observation symbols $$M$$
2. Length of observation seqences $$T$$ and
3. Specification of three probability measures $$ \mathbf{A}, \mathbf{B}$$ and $$\mathbf{\pi} $$

The set of all HMM model parameters is represented by $$\mathbf{\lambda} =(\pi, A, B)$$.

Since $$S$$ is not observed, the likelihood function $$Y$$ is given by the joint distribution of $$Y$$ and $$S$$ over all possible state. 

 $$
P(Y \mid \lambda) = \sum P(Y, S \mid  \lambda)
 $$

where 

$$
P(Y,S \mid \lambda) = P(Y \mid S,\lambda)P(S \mid \lambda)
$$

Note that $$y_t$$ is independent and identically distributed given state sequence $$S = \{s_1, s_2....,s_N\}$$. Also each state at time $$t$$ depend on the state at its previous time $$t-1$$. Then

 $$
P(Y \mid S, \lambda) = \prod_{t=1}^T P(y_t \mid s_t)
 $$

Similary

 $$
P(S \mid \lambda) = \pi _{s_1} \prod _{t=2}^T a_{ij}
 $$

The joint probability is therefore:

 $$
P(Y \mid \lambda) = \pi _{s_1}P(y_1 \mid s_1) \sum \prod_{t=2}^T a_{ij} P(y_t \mid s_t)
 $$

## Three main problems in HMMs

When applying HMM to a real world problem, three important problem must be solved. 

1. Evaluation Problem: Given HMM parameters $$\lambda$$ and the observation seqence $$Y = \{Y_1, Y_2....,Y_M\}$$, find $$P(Y \mid \lambda)$$ the likelihood of the observation sequence $$Y$$ given the model $$\lambda$$. This problem give a score on how well a given model matches a given observation and thus allows you to choose the model that best match the observation.
2. Decoding Problem: Given HMM parameters $$\lambda$$ and the observation seqence $$Y = \{Y_1, Y_2....,Y_M\}$$, find an optimal state sequense $$S = \{S_1, S_2....,S_N\}$$ which best explain the observation.This problem attempt to cover the hidden part of the model.
3. Learning Problem: Given the obseravtion seqence $$Y = \{Y_1, Y_2....,Y_M\}$$, find the model parameters $$\lambda$$ that maximize $$P(Y \mid \lambda)$$.This problem attempt to optimize the model parameters so as to describe the model.

The first and the second problem can be solved by the dynamic programming algorithms known as the Viterbi algorithm and the Forward-Backward algorithm, respectively. The last one can be solved by an iterative Expectation-Maximization (EM) algorithm, known as the Baum-Welch algorithm. We will discuss the first and the second problem in this post.


## Solution to Problem 1

A straight forward way to solve this problem is to find $$P(Y \mid S, \lambda)$$ for fixed state sequences $$S = \{s_1,...s_T \}$$ and then sum up over all possible states. This is generally infeasible since it requires about $$2TN^T$$ multiplications. However this problem can be efficiently solved by using the forward algorithm  as follows:
 
### The forward-backward Algorithm
 
Let us define the **forward variable** 

$$
\alpha _t(i)=P(y_1,\ldots y_t, s_t=i \mid \lambda)
$$

the probability of the partial observation sequences $$y_1 \ldots y_t$$  up to time $$t$$ and the state $$s_t =i$$ at time $$t$$ given the model $${\lambda}$$. We also define an emission probability given HMM state $$i$$ at time $$t$$ as $$b_i(y_t)$$.

#### Forward-Algorithm

**Initilization** 

Let

$$
\begin{aligned}
\alpha _1(i)&=P(y_1, s_1=i \mid \lambda) \\
    & = P(y_1 \mid s_1=i,\lambda)P(s_1=i \mid \lambda)\\
    &= \pi _i b_i(y_1) \text{  for  } 1\leq i \geq N
\end{aligned}
$$

**Induction**

For $$t=2,3...T$$ and $$1\leq i \geq N$$, compute:

$$
\begin{aligned}
\alpha _{t}(i) & = P(y_1 \ldots y_t, s_t=i \mid \lambda)\\
 &= \displaystyle \sum_{j=1}^{N} P(y_1 \ldots y_{t}, s_{t-1}=j,s_t=i \mid \lambda) \\
 &= \displaystyle \sum_{j=1}^{N} P(y_t \mid s_t=i, y_1,\ldots y_{t-1}, s_{t-1}=j, \lambda) \\
   &  \times P(s_t=i \mid y_1 \ldots y_{t-1} \ldots , s_{t-1}=j, \lambda) \\
   & \times P(y_1 \ldots y_{t-1}, s_{t-1}=j,\lambda) \\
 & = P(y_t \mid s_t=i,\lambda)\displaystyle \sum_{j=1}^{N} P(s_t=i \mid s_{t-1}=j)\cdot P(y_1, \ldots y_{t-1}, s_{t-1}) \\
& = b_i(y_{t})\displaystyle \sum_{j=1}^{N} \alpha _{t-1}(i)a_{ij}  
\end{aligned}
$$


**Termination**

From $$\alpha _t(i)=P(y_1,...y_t, s_t=i \mid \lambda)$$, it cear that:

$$ 
\begin{aligned} 
P(Y \mid \lambda) &= \displaystyle \sum_{i=1}^{N} P(y_1,\ldots y_T, s_T = i \mid \lambda) \\
&= \displaystyle \sum_{i=1}^{N}\alpha _T(i)  
\end{aligned}
$$


The forward algorithm only requires about $$N^2T$$ multiplications and is it can be implemented in python as follows.

```python
def forward(obs_seq):
        T = len(obs_seq)
        N = A.shape[0]
        alpha = np.zeros((T, N))
        alpha[0] = pi*B[:,obs_seq[0]]
        for t in range(1, T):
            alpha[t] = alpha[t-1].dot(A) * B[:, obs_seq[t]]
        return alpha

 def likelihood(obs_seq):
        # returns log P(Y  \mid  model)
        # using the forward part of the forward-backward algorithm
        return  forward(obs_seq)[-1].sum()      
```


### Backward Algorithm

This is the same as the forward algorithm discussed in the previous sectionexcept that it start at the end and works backward toward the beginning. We first define the **backward variable** $$\beta_t(i)=P(y_{t+1},y_{t+2} \ldots y_{T} \mid s_t=i, {\lambda})$$: probability of the partial observed sequence from $$t+1 $$ to the end at $$T$$ given state $$i$$ at time $$t$$ and the model $$\lambda$$. 

Then $$\beta_t(i)$$ can be computed recursively as follows.

**Initilization**

Let $$\beta_{T}(i)= 1$$, for $$1 \leq i\geq N$$

**Induction**

For $$t =T-1, T-2,\ldots1$$ for $$1 \leq i\geq N$$ and by using the sum and product rules, we can rewrite $$\beta_t(j)$$ as:

$$
\begin{aligned}
\beta_t(i)&=P(y_{t+1},\ldots y_{T} \mid s_t=j, {\lambda}) \\
 &= \displaystyle \sum_{i=1}^{N} P(y_{t+1} \ldots y_T, s_{t+1}=i \mid s_t=j, \lambda) \\
 & = \displaystyle \sum_{i=1}^{N} P(y_{t+1} \ldots y_T, s_{t+1}=i, s_t=j, \lambda)\cdot P(s_{t+1}=i \mid s_t=j) \\
 &= \displaystyle \sum_{i=1}^{N} P(y_{t+2} \ldots y_T, s_{t+1}=i, \lambda)\cdot P(y_{t+1} \mid s_{t + 1}=i, \lambda)\cdot P(s_{t+1}=i \mid s_t=j) \\
 & = \displaystyle \sum_{i=1}^{N} a_{ij}b_i(y_{t+1})\beta _{t+1}(i)
\end{aligned}
$$

**Termination**

$$
\begin{aligned}
\beta_{0} & = P(Y \mid \lambda) \\
& = \displaystyle \sum_{i=1}^{N} P(y_1,\ldots y_T, s_1=i) \\
&= \displaystyle \sum_{i=1}^{N} P(y_1,\ldots y_T \mid s_1=i)\cdot P(s_1=i) \\
& = \displaystyle \sum_{i=1}^{N} P(y_1 \mid s_1=i)\cdot P(y_2,\ldots y_T \mid s_1=i)\cdot P(s_1=i) \\
& = \displaystyle \sum_{i=1}^{N} \pi _i b_i(y_1)\beta _1(i)
\end{aligned}
$$


Python implementation of forward algorithm is as shown below;

```python
def backward(obs_seq):
        N = A.shape[0]
        T = len(obs_seq)

        beta = np.zeros((N,T))
        beta[:,-1:] = 1

        for t in reversed(range(T-1)):
            for n in range(N):
                beta[n,t] = np.sum(beta[:,t+1] * A[n,:] * B[:, obs_seq[t+1]])

        return beta
```

#### Posterior Probability
The forward variable $$\alpha _t(i)$$ and backward variable $$\beta _t(i)$$ are used to calculate the posterior probability of a specific case. Now for $$t=1...T$$ and $$i=1..N$$, let define posterior probability $$\gamma_t(i)=P(s_t=i \mid Y, \lambda)$$ the probability of being in state $$s_t = i$$ at time $$t$$ given the observation $$Y$$ and the model $$\lambda$$.

$$
\begin{aligned}
\gamma_t(i) & = \frac{P(s_t=1, Y \mid \lambda)}{P(Y \mid \lambda)} \\
 &=\frac{P(y_1,\ldots y_t, s_t=1, \mid \lambda)}{P(Y \mid \lambda)}
\end{aligned}
$$

Consider:

$$
\begin{aligned}
P(y_1,\ldots y_t, s_t=1, \mid \lambda) & = P(y_1,\ldots y_t \mid  s_t=1,\lambda)\cdot P(y_{t+1},\ldots y_T \mid  s_t=1,\lambda)\cdot P(s_t =i  \mid \lambda) \\
 & = \alpha _t(i) \cdot \beta _t(i)
\end{aligned}
$$

Thus 

$$
\gamma_t(i) = \frac{\alpha _t(i) \cdot \beta _t(i)}{P(Y \mid \lambda)}
$$

where 

$$ 
P(Y \mid {\lambda}) =  \displaystyle \sum_{i=1}^{N}\alpha _T(i)
$$

In python:

```python
def gamma(obs_seq):
    alpha = forward(obs_seq)
    beta  = backward(obs_seq)
    obs_prob = likelihood(obs_seq)
    return (np.multiply(alpha,beta.T) / obs_prob)
```

We can use $$\gamma_t(i)$$ to find the most likely state at time $$t$$ which is the state $$s_t=i$$ for which $$\gamma_t(i)$$ is maximum. This algorithm [works fine in the case when HMM is ergodic](http://www.shokhirev.com/nikolai/abc/alg/hmm/hmm.html) i.e. there is transition from any state to any other state. If applied to an HMM of another architecture, this approach could give a sequence that may not be a legitimate path because some transitions are not permitted. To avoid this problem *Viterbi algorithm* is the most common decoding algorithms used.


### Viterbi Algorithm

Viterbi is a kind of dynamic programming algorithm that make uses of a dynamic programming trellis.

The virtebi algorithm offer an efficient way of finding  the single best state sequence.Let define the highest probability along a single path, at time $$t$$, which accounts for the first $$t$$ observations and ends in state $$j$$ using a new notation:

$$
\begin{aligned}
\delta_t(i) & = \max_{s_1,\ldots s_{t-1}} P(s_1, \ldots s_t =1, y_1,\ldots y_t \mid \lambda)
\end{aligned}
$$

By induction, a recursive formula of $$\delta_{t+1}(i)$$ from $$\delta_t(i)$$  is derived to calculate this probability as follows:

Consider the joint distribution appearing in $$\delta_{t+1}(i)$$, which can be rewritten when $$s_{t+1}=i$$ and $$s_t = j$$ as:

$$
\begin{aligned}
P(s_1,\ldots, s_t=j,s_{t+1}=i, y_1,\ldots y_t, y_{t+1} \mid \lambda) & = P(s_1 \ldots s_t=j, y_1,\ldots y_t  \mid \lambda)\\& \times P(s_{t+1}=i,y_{t+1} \mid s_1, \ldots s_t, y_1, \ldots y_t, \lambda) \\
 & = P(s_1 \ldots s_t=j, y_1,\ldots y_t  \mid \lambda)\cdot P(s_{t+1} \mid s_t, \lambda)\\ & \times P(y_{t+1} \mid s_{t+1},\lambda) \\
  & = P(s_1 \ldots s_t=j, y_1,\ldots y_t  \mid \lambda)\cdot a_{ij}b_i(y_{t+1})
\end{aligned}
$$


Thus $$\delta_{t+1}(i)$$  is computed recursively from $$\delta_{t+1}(j)$$ as:

$$
\begin{aligned}
\delta_{t+1}(i) &= \max_{s_1,\ldots s_{t}=j} P(s_1 \ldots s_t=j, y_1,\ldots y_t  \mid \lambda)\cdot a_{ij}b_i(y_{t+1}) \\
 & = \max_{j}\Big[ \delta_t(j) a_{ij}\Big]\cdot b_i(y_{t+1})
\end{aligned}
$$

We therefore need to keep track the state that maximize the above equation so as to backtrack to the single best state sequence in the following Viterbi algorithm:

**Initilization**

For $$1 \leq i \geq N$$, let:

$$
\begin{aligned}
\delta _1(i)&= \pi _{s_i}b_i(y_1)\\
\Theta _1(i)&=0
\end{aligned}
$$ 


**Recursion**

Calculate  the ML (maximum likelihood) state sequences and their probabilities. For $$t=2,3,...T$$ and $$1\leq i \geq N$$ 

$$
\begin{aligned}\delta_t(i) & = \displaystyle \max_{j\epsilon{1,..N}} \Big[\delta_{t-1}(j)a_{ij}\Big]\cdot b_i(y_t) \\
\Theta_t(i) & = \arg\max_j \Big[\delta_{t-1}(j)a_{ij} \Big] 
\end{aligned}
$$  

    
**Termination**:

Retrieve the most likely final state 

$$
\begin{aligned} \hat{P} &= \displaystyle \max_{j\epsilon{1,..N}}[\delta_T(j)]  \\
\hat{S}_T & = \arg\max_j [\delta_T(j)]
\end{aligned}
$$


**State sequence backtracking**: 

Retrieve  the most likely state sequences (virtebi path)

$$
  \hat{S}_t = \Theta_{t+1}(\hat{S}_{t+1}) \text{, where } t=T-1,T-2,\ldots1
$$ 

Virtebi algorithm uses the same schema as the Forward algorithm except for two differences:

1. It uses maximization in place of summation at the recursion and termination steps.
2. It keeps track of the arguments that maximize $$\delta_t(i)$$ for each $$t$$ and $$i$$, storing them in the N by T matrix $$\Theta$$. This matrix is used to retrieve the optimal state sequence at the backtracking step.

Python implementation of virtebi algorithm 
```python
def viterbi(obs_seq):
        # returns the most likely state sequence given observed sequence x
        # using the Viterbi algorithm
        T = len(obs_seq)
        N = A.shape[0]
        delta = np.zeros((T, N))
        psi = np.zeros((T, N))
        delta[0] = pi*B[:,obs_seq[0]]
        for t in range(1, T):
            for j in range(N):
                delta[t,j] = np.max(delta[t-1]*A[:,j]) * B[j, obs_seq[t]]
                psi[t,j] = np.argmax(delta[t-1]*A[:,j])

        # backtrack
        states = np.zeros(T, dtype=np.int32)
        states[T-1] = np.argmax(delta[T-1])
        for t in range(T-2, -1, -1):
            states[t] = psi[t+1, states[t+1]]
        return states
```

To summarize, we can compute the following from HMM:

1. The marginalized likelihood function $$P(Y \mid \lambda)$$ from the forward or backward algorithm.
2. The posterior probability $$\gamma_t(i) = P(s_t=i  \mid Y, \lambda)$$ from the forward–backward algorithm.  
3. The optimal state sequence $$ \hat{S} = \max_{s} P(S \mid Y, \lambda) = \max_{s} P(S, Y \mid  \lambda)$$from the Viterbi algorithm.
4. The segmental joint likelihood function $$P(\hat{S},Y \mid \lambda)$$ from the Viterbi algorithm.

These values are used in the decoding step and the training step of estimating model parameters $$\lambda$$.

**Example 1**

Conside the Bob-Alice example as described [here](https://en.wikipedia.org/wiki/Hidden_Markov_model#A_concrete_example). Two friends, Alice and Bob, who live far apart from each other and who talk together daily over the telephone about what they did that day. Bob is only interested in three activities: walking in the park, shopping, and cleaning his apartment. The choice of what to do is determined exclusively by the weather on a given day. Alice has no definite information about the weather where Bob lives, but she knows general trends. Based on what Bob tells her he did each day, Alice tries to guess what the weather must have been like. 

Alice believes that the weather operates as a discrete Markov chain. There are two states, "Rainy" and "Sunny", but she cannot observe them directly, that is, they are hidden from her. On each day, there is a certain chance that Bob will perform one of the following activities, depending on the weather: "walk", "shop", or "clean". Since Bob tells Alice about his activities, those are the observations.

```python
states = ('Rainy', 'Sunny')
observations = ('walk', 'shop', 'clean')
pi = np.array([0.6, 0.4])  #initial probability 
A = np.array([[0.7, 0.3],[0.4, 0.6]]) #Transmission probability 
B = np.array([[0.1, 0.4, 0.5],[0.6, 0.3, 0.1]]) #Emission probability
```

Suppose Bob says walk, clean, shop, shop, clean, walk. What will Alice hears.

```python
bob_says = np.array([0, 2, 1, 1, 2, 0])
alice_hears=viterbi(bob_says)
print("Bob says:", ", ",list(map(lambda y: observ_bob[y], bob_says)))
print("Alice hears:", ", ", list(map(lambda s: states_bob[s], alice_hears)))
```

<blockquote>
    ('Bob says:', 'walk, clean, shop, shop, clean, walk')
    ('Alice hears:', 'Sunny, Rainy, Rainy, Rainy, Rainy, Sunny')
</blockquote>

The notebook with codes for the above example can be found in [here](https://github.com/sambaiga/HMM/blob/master/HMM%20Basics.ipynb)


### References
1.  L. R. Rabiner, [A tutorial on hidden Markov models and selected applications in speech recognition](http://www.cs.ucsb.edu/~cs281b/papers/HMMs%20-%20Rabiner.pdf), Proceedings of the IEEE, Vol. 77, No. 2, February 1989.
2.  Shinji Watanabe, Jen-Tzung Chien, [Bayesian Speech and Language Processing](https://books.google.co.tz/books/about/Bayesian_Speech_and_Language_Processing.html?id=rEzzCQAAQBAJ&printsec=frontcover&source=kp_read_button&redir_esc=y#v=onepage&q&f=false), Cambridge University Press, 2015.
3. [Viterbi Algorithm in Speech Enhancement and HMM](http://www.vocal.com/echo-cancellation/viterbi-algorithm-in-speech-enhancement-and-hmm/)
4. Nikolai Shokhirev, [Hidden Markov Models](http://www.shokhirev.com/nikolai/abc/alg/hmm/hmm.html)