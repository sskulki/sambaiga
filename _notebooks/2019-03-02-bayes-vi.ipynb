{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Inference (VI)\n",
    "> The post introduce the basics principle of bayesian varitaional inference  as one of the approach used to approximate difficult probability distribution, derive the ELBO function for variational inference and discussed about Mean Field Variational Inference (MFVI) and the Coordinate Ascent Variational Inference (CAVI) algorithms.\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [jupyter]\n",
    "- image: images/vi.jpg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian method offer a different paradigm for doing statistical analysis. It is practical method for making inferences from data using probability models. Unlike other statistical approach, bayesian models are easy to interpret and incorporate uncertainty. In bayesian method  we start with a belief which is also called a **prior**. We then update our belief after observing some data. The outcome is called a **posterior**. The process repeats as we keep on observing more data where the old *posterior* becomes a new *prior*. The  process employs the Bayes rule. \n",
    "\n",
    "Consider the Bayesian theorem, which allows us to use some knowledge or belief that we already have. Given data point $\\mathcal{D} = \\{x \\in \\mathbb{R}^{N\\times d}, y \\in \\mathbb{R}^{N\\times c}\\}$. The Bayesian  approach  treat the latent variable or parameter $z$ as random variable with some prior distribution $p(z)$. This is the probability of parameters $z$ before hand.\n",
    "\n",
    "$$\n",
    "p(z | \\mathcal{D}  ) = \\frac{p(\\mathcal{D} | z )\\cdot p(z)}{p(\\mathcal{D})}\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "p(\\mathcal{D}) = \\int p(\\mathcal{D} |z )\\cdot p(z) dz\n",
    "$$\n",
    "\n",
    "From the bayesian theorem above,  $z$ is the hypothesis about the world, and $$\\mathcal{D}$$ is the data or evidence. The probability $p(\\mathcal{D} \\mid z)$ is called  **likeli-hood**; the probability of data given the latent variable and $p(\\mathcal{D})$ is the **marginal-likelihood** and $p(z \\mid \\mathcal{D}  )$ is the **posterior**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Inference\n",
    "\n",
    "Given data set $\\mathcal{D}$ and latent variable $z$ that relate $x$ and $y$ such that:\n",
    "$$\n",
    "y = f_{z}(x:z)\n",
    "$$\n",
    "The first step in bayesian inference is to identify the parameter $z$ and express our lack of knowledge  about this parameter in term of probability distribution $p(z)$. This is the prior knowledge about the parameter $z$. After that we express a *likelihood*  $p(\\mathcal{D} \\mid z)$ which tell us how the data $\\mathcal{D}$ interact with  parameter $z$. Together the prior and the likelihood make our model (generative model). It tell us how we can simulate from our data.\n",
    "\n",
    "In **training** stage we apply Bayesian theorem to get posterior distribution:\n",
    "\n",
    "$$\n",
    "p(z|\\mathcal{D}) = \\frac{p(\\mathcal{D}|, z)}{p(\\mathcal{D})}\n",
    "$$ \n",
    "\n",
    "In testing stage we find **predictive-distribution**\n",
    "\n",
    "$$\n",
    "p(\\hat{y}| x, \\mathcal{D}) = \\int p(\\hat{y} | x, z)\\cdot p(z | x, y) dz\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y= f_{\\theta}(x: \\theta) + \\in\n",
    "$\n",
    " where $\\in$ is the noise due to measurement and \n",
    "$f_{\\theta}(X: \\theta)$ is the hypothesis function given;\n",
    "$\n",
    "f_{\\theta}(X: \\theta) = b + \\sum_{i=1}^N w_i \\phi (x_i) = \\theta^T\\cdot \\phi(X)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\phi(X)$ is the basis function and $\\theta$ is the model parameters such that \n",
    "$\n",
    "\\theta_{0} =b$$  and $$\\phi_0=1\n",
    "$\n",
    "\n",
    "The output of this model is the single point estimate for the best model parameter. The Bayesian modelling approach to this problem offer a systematic framework for learning distribution over values of the parameters and not a single estimate. The bayesian linear regression model $y= f_{\\theta}(x: \\theta) + \\in\n",
    "$ as a Gaussian  distribution such that:\n",
    "$\n",
    "p(y|x, \\theta) = \\mathcal{N}(y|f_{\\theta}(x: \\theta), \\beta^{-1})\n",
    "$\n",
    "\n",
    "Assuming the data point are drawn independently and identically distributed the likelihood is expressed as:\n",
    "\n",
    "$$\n",
    "p(Y| X, \\theta) = \\prod_{i=1}^N \\mathcal{N}(y_i|f_{\\theta}(x_i: \\theta_i), \\beta^{-1})\n",
    "$$\n",
    "\n",
    "Let choose a prior that is conjugate to the likelihood \n",
    "\n",
    "$$\n",
    "p(\\theta|X) = \\mathcal{N}(\\theta|0, \\alpha^{-1})\n",
    "$$\n",
    "\n",
    "Thus the posterior is given as:\n",
    "\n",
    "$$\n",
    "p(\\theta|Y, X) \\propto \\mathcal{N}(\\theta|0, \\alpha^{-1})\\cdot \\prod_{i=1}^N \\mathcal{N}(y_i|f_{\\theta}(x_i: \\theta_i), \\beta^{-1})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Inference (VI)\n",
    "\n",
    "In the previous section we show that inference in probabilist model is often intractable and introduced several approach used to approximate the inference. Variational Inference (VI) is one of the approach used to approximate difficult probability distribution by turning the calculation about model into optimization. \n",
    "\n",
    "Consider a probabilistic model which is joint distribution $p(x,z)$ of the latent variable  $$z$$ observed variables $x$. To draw inference on the latent variable $z$ we compute the posterior\n",
    "\n",
    "$$\n",
    "p(z|x) = \\frac{p(x,z)}{p(x)} = \\frac{p(x|z)\\cdot p(z)}{p(x)}\n",
    "$$\n",
    "\n",
    "where \n",
    "$\n",
    "p(x)=\\int p(x|z)\\cdot p(z) dz\n",
    "$\n",
    "To approximate $p(z\\mid x)$ we first choose an approximating family of distribution $q(z)$ over latent variable  $z$. Then we find set of parameters that makes $q(z)$ close to posterior distribution $p(z\\mid {\\bf x})$. Thus VI approximate $p(z\\mid x)$ with new distribution $q(z)$ such that $q(z)$ is close to $p(z\\mid x)$. To achieve this we minimize KL divergence between $q(z)$ and $p(z\\mid x)$ such that:\n",
    "\n",
    "$$\n",
    "q^*(z) = \\arg \\min D_{KL}(q(z)||p(z|x))\n",
    "$$\n",
    "\n",
    "where \n",
    "$\n",
    "D_{KL}(q(z)||p(z|x)) = \\int q(z)\\log \\frac{q(z)}{p(z|x)}\n",
    "$\n",
    "\n",
    "It clear that we can not minimize KL divergence since it is directly depend on posterior $p(z\\mid x)$. However we can minimize a function that is equal to KL divergence plus constant. This function is called **Evidence Lower Bound**(ELBO) $\\mathcal{L}_{VI}(q)$.\n",
    "\n",
    "### Evidence Lower Bound (ELBO)\n",
    "\n",
    "To derive the ELBO we first consider the [Jensen's inequality](https://en.wikipedia.org/wiki/Jensen%27s_inequality) which relates the value of a convex function of an integral to the integral of the convex function such that $f(\\mathbb{E}[x]) \\geq \\mathbb{E}[f(x)]$ where $f(.)$ is the concave function. Since logarithmic are strictly concave function it is clear that\n",
    "\n",
    "$$\n",
    "\\log \\int p(x)g(x) dx \\geq \\int p(x)\\log g(x)\n",
    "$$\n",
    "\n",
    "Let us consider a log of marginal evidence.\n",
    "\n",
    "$$\n",
    "\\begin{aligned} \n",
    "\\log p(x) & = \\log \\int_z p(x,z) dz\\\\\n",
    "          & =\\log \\int_z p(x,z)\\cdot \\frac{q(z)}{q(z)} dz \\\\\n",
    "          & =\\log \\int_z q(z)\\frac{p(x,z)}{q(z)} dz\\\\\n",
    "          & =\\log \\left(\\mathbb{E}_q[\\frac{p(x,z)}{q(z)}] \\right)\\\\\n",
    "          &\\geq \\mathbb{E}_q[ \\log p(x,z)] - \\mathbb{E}_q[\\log q(z)]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The final line is the ELBO which is the lower bound for the evidence. Thus the evidence lower bound for probability model $$p(x,z)$$ and approximation $$q(z)$$ to the posterior is\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{VI}(q) = \\mathbb{E}_q[ \\log p(x,z)] - \\mathbb{E}_q[\\log q(z)]\n",
    "$$\n",
    "\n",
    "We can now show that KL divergence to the posterior is equal to the negative ELBO plus constant.\n",
    "\n",
    "$$\n",
    "\\begin{aligned} \n",
    "D_{KL}(q(z)||p(z|x)) &= \\int q(z)\\log \\frac{q(z)}{p(z|x)}\\\\\n",
    "                     &= \\mathbb{E}_q[\\log q(z)] - \\mathbb{E}_q[\\log p(x,z)] + \\mathbb{E}_q[\\log p(x)]\\\\\n",
    "                     &=-\\left(\\mathbb{E}_q[\\log p(x,z)] - \\mathbb{E}_q[\\log q(z)] \\right) +\\log p(x)\\\\\n",
    "                     &= -\\mathcal{L}_{VI}(q) +\\log p(x)\\\\\n",
    "\\mathcal{L}_{VI}(q) &=\\log p(x) + D_{KL}(q(z)||p(z|x))\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "From the equation above it clear that minimizing the KL divergence is equivalent to maximizing the ELBO. Recall that we want to find $$q(z)$$ such that KL divergence is small, the variational objective function becomes\n",
    "\n",
    "$$\n",
    "q^*(z) = \\arg \\min D_{KL}(q(z)||p(z|x)) = \\arg \\max \\mathcal{L}_{VI}(q)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### Mean Field Variational Inference\n",
    "\n",
    "One of the important question on VI, is how to construct the family of variational distributions from which we want to draw $q(z)$ from. The simplest family is where each latent parameter $z_i$ has its own\n",
    "independent distribution. This means that we can easily factorize the variational distributions into groups:\n",
    "\n",
    "$$\n",
    "q(z_1, \\ldots, z_m) = \\prod_{i=1}^m q(z_i)\n",
    "$$\n",
    "\n",
    "This family of distribuion are known as Mean-Field Variational Family that make use of [mean field theory](https://en.wikipedia.org/wiki/Mean_field_theory). Inference using this factorization is known as Mean-Field Variational Inference (MFVI). \n",
    "\n",
    "It possible to further parameterize the approximating distributions $q(z)$ with variational parameters $\\lambda$ such that the approximating distribution become $q(z_i ; \\lambda_i)$. For example if we set our family of approximating distributions as a set of\n",
    "independent gauasian distributions $\\mathcal{N}(\\mu_i, \\sigma^2_i)$ and parameterize this distributions with the mean and variance where $\\lambda_i = (\\mu_i, \\sigma^2_i)$ is the set of variational parameters.\n",
    "\n",
    "The common algorithms used in practise to do VI under mean filed assumptions are coordinate ascent optimization (CAVI) and stochastic gradient based method. \n",
    "\n",
    "### Coordinate Ascent Variational Inference (CAVI)\n",
    "\n",
    "The CAVI algorithm derive variational updates by hand and perform coordinate ascent (iteratively updating each latent variable $z_i$) on the latent until convergence of the ELBO. A common procedure to conduct CAVI is:\n",
    "\n",
    "- Choose variational distributions $q(z)$\n",
    "- Compute ELBO;\n",
    "- Optimize individual $q(z_i)$ ’s by taking the gradient for each latent variable;\n",
    "- Repeat until ELBO converges.\n",
    "\n",
    "The coordinate ascent update for a latent variable can be derived by maximizing the ELBO function above. First, recall ELBO\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{VI}(q) = \\mathbb{E}_q[ \\log p(x,z)] - \\mathbb{E}_q[\\log q(z)]\n",
    "$$ \n",
    "Applying chain rule we can decomopse the joint  $p(x,z)$ as;\n",
    "$$\n",
    "p(x_{1:n}, z_{1:m}) = p(x_{1:n}) \\prod_{i=1}^m p(z_i|z_{1:(i-1)}, x_{1:n})\n",
    "$$\n",
    "Using mean field approximation, we can decompose the entropy term of the ELBO as\n",
    "$$\n",
    "\\mathbb{E}_q[\\log q(z)] = \\sum_{i=1}^m \\mathbb{E}_q[\\log q(z_i)]\n",
    "$$\n",
    "Under the above assumption the ELBO become:\n",
    "$$\n",
    "\\mathcal{L}_{ELBO}(q) = \\log p(x_{1:n}) + \\sum_{i=1}^m \\mathbb{E}_q[\\log p(z_i|z_{1:(i-1)}, x_{1:n}) ] - \\mathbb{E}_q[\\log q(z_i)\n",
    "$$\n",
    "\n",
    "To find this $\\arg \\max_{q(z_i)} \\mathcal{L}_{ELBO}(q)$ we take derivative of ELBO with respect to $q(z_i)$ using Lagrange multipliers and set the derivative to zero. It can be shown that the coordinate ascent update rule is equal to \n",
    "\n",
    "$$\n",
    "q^*(z_i) \\propto \\{  \\mathbb{E}_{q-i}[\\log q(z_i,z_{\\neg},x)]\\}\n",
    "$$\n",
    "\n",
    "where the notation $\\neg$ denotes all indices other than the $i^{th}$\n",
    "\n",
    "Despite being very fast  method for some models  only  work with  conditionally conjugate models. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "\n",
    "1. [ICML 2018 tutorial](http://www.tamarabroderick.com/tutorial_2018_icml.html):Variational Bayes and Beyond: Bayesian Inference for Big Data.\n",
    "2. [Shakir Mohamed](https://www.shakirm.com/papers/VITutorial.pdf):Variational Inference  for Machine Learning. \n",
    "3. [DS3 workshop](https://emtiyaz.github.io/teaching/ds3_2018/ds3.html):Approximate Bayesian Inference: Old and New.\n",
    "4. [Variational Inference and Deep Generative Models](https://github.com/philschulz/VITutorial):Variational Inference for NLP audiences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
