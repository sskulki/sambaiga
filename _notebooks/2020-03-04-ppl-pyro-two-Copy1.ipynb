{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of Probability and Information Theory\n",
    "> This post present how to vectorize pyro codes\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [jupyter]\n",
    "- image: images/probabilities.jpg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Probability and Information theory are important field that has made significant contribution to deep learning and AI. Probability theory allows us to make *uncertain statements* and to *reason* in the presence of *uncertainty* where information theory enables us to *quantify* the amount of *uncertainty* in a probability distribution.\n",
    "\n",
    "## Probability Theory\n",
    "\n",
    "Probability is a mathematical framework for representing uncertainty. It is very applicable in Machine learning and Artificial Intelligence as it allows to make *uncertain statements* and  *reason* in the presence of *uncertainty*. Probability theory allow us to design ML algorithms that take into consideration  of *uncertain* and sometimes *stochastic*  quantities. It further tell us tell us how ML systems should *reason* in the presence of uncertainty. This  is necessary because most things in the world  are uncertain, and thus  ML systems should reason using probabilistic rules. Probability theory can also be used to analyse the behaviour of  ML algorithms probabilistically. Consider evaluating ML classification algorithm using  accuracy metric which is  the probability that the model will give a correct prediction  given an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability and Probability distribution\n",
    "\n",
    "**Probability** is a measure of the likelihood that an event will occur in a random experiment. It is quantified as number between 0 and 1. The mathematical function that maps all possible outcome of a random experiment with its associated probability it is called **probability distribution**. It describe how likely a random variable or set of random variable is to take on each of its possible state. The probability distribution for discrete random variable is called probability mass function (PMF) which measures the probability $X$ takes on the value $x$, denoted denoted as $P(X=x)$.\n",
    "To be PMF on random variable $X$  a function $P(X)$ must satisfy:\n",
    "\n",
    "- Domain of $P$ equal to all possible states of $X$\n",
    "- $\\forall x \\in X, 0\\leq P(X=x) \\leq 1$\n",
    "- $\\sum_{x \\in X} P(x) =1$\n",
    "\n",
    "Popular and useful PMF includes poison, binomial, bernouli, and uniform. Let consider a poison  distribution defined as:\n",
    "\n",
    "$$\n",
    "P(X=x) = \\frac{\\lambda ^x e^{ -\\lambda}}{x!}\n",
    "$$\n",
    "\n",
    "$\\lambda >0$ is called a parameter of the distribution, and it controls the distribution's shape. By increasing $\\lambda$ , we add more probability to larger values, and conversely by decreasing $\\lambda$  we add more probability to smaller values as shown in figure below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pmf of poison distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of a PMF, a continuous random variable has a probability density function (pdf) denoted as $f_X(x)$. An example of continuous random variable is a random variable with exponential density. \n",
    "$$\n",
    "f_X(x\\mid \\lambda) = \\lambda ^x e^{ -\\lambda} \\text{,  } x \\geq 0\n",
    "$$\n",
    "\n",
    "\n",
    "To be a probability density function $p(x)$ must satisfy\n",
    "- The domain of $p$ must be the set of all possible state\n",
    "- $\\forall x \\in X, f_X(x) \\geq 0$\n",
    "- $\\int_{x \\in X} f_X(x)dx =1$\n",
    "\n",
    "The pdf does not give the probability of a specific state directly. The probability that $x$ is between two point $a, b$ is \n",
    "\n",
    "$\\int_{a}^b f_X(x)dx$\n",
    "\n",
    "The probability of intersection of two or more random variables is called *joint probability* denoted  as $\n",
    "P(X, Y)\n",
    "$\n",
    "\n",
    "Suppose we have two random variable $X$ and $Y$ and we know the joint PMF or pdf distribution between these variable. The PMF or pdf  corresponding to a single variable is called *marginal probability distribution* defined as\n",
    "$$\n",
    "P(x) = \\sum_{y\\in Y} P(x, y)\n",
    "$$\n",
    "\n",
    "for discrete random variable and \n",
    "$$\n",
    "p(x) = \\int p(x)dy\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marginalization allows us to get the distribution of variable $$X$$ ignoring variable $Y$ from the joint distribution $P(X,Y)$. The probability that some event will occur given we know other events is called condition probability denoted as $P(X\\mid Y)$. The  marginal, joint and conditional probability are linked by the following rule \n",
    "$\n",
    "P(X|Y) = \\frac{P(X, Y)}{P(Y)}\n",
    "$\n",
    "\n",
    "**Independence, Conditional Independence and Chain Rule**\n",
    "\n",
    "Two random variables are said to be independent of each other if the probability that one random variables occur in no way affect the probability of the other random variable occurring. $X$ and $Y$ are said to be independent if $P(X,Y) = P(X)\\cdot P(Y)$\n",
    "On the other hand two random variable $X$ and $Y$ are conditionally independent given an event $Z$ with $P(Z)>0$ if \n",
    "\n",
    "$$\n",
    "P(X,Y\\mid Z) = P(X\\mid Y)\\cdot P(Y\\mid Z)\n",
    "$$\n",
    "\n",
    "The good example of conditional independence can be found on this [link](). Any joint probability distribution over many random variables may be decomposed into conditional distributions using *chain rule* as follows:\n",
    "\n",
    "$$\n",
    "P(X_1,X_2, \\ldots, X_n ) = P(X_1)\\prod_{i=2}^n P(X_i\\mid X_i, \\ldots X_{i-1})\n",
    "$$\n",
    "\n",
    "**Expectation, Variance and Covariance**\n",
    "\n",
    "Expected value of some function $f(x)$ with respect to a probability distribution $P(X)$ is the average or mean value that $f(x)$ takes on when $x$ is drawn from $P$.\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{x\\sim P}[f(x)] = \\sum P(x).f(x)\n",
    "$$\n",
    "\n",
    "for discrete random variable and \n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{x\\sim P}[f(x)] = \\int P(x).f(x)dx\n",
    "$$\n",
    "\n",
    "Expectation are linear such that \n",
    "$$\n",
    "\\mathbb{E}_{x\\sim P}[\\alpha \\cdot f(x) + \\beta \\cdot g(x)] = \\alpha \\mathbb{E}_{x\\sim P}[f(x)] + \\beta \\mathbb{E}_{x\\sim P}[g(x)]\n",
    "$$\n",
    "\n",
    "Variance is a measure of how much the value of a function of random variable $X$ vary as we sample different value of $x$ from its probability distribution.\n",
    "$$\n",
    "Var(f(x)) =\\mathbb{E}([f(x)-\\mathbb{E}[f(x)]^2])\n",
    "$$\n",
    "The square root of the variance is know as standard deviation. On the other hand the covarince give some sense of how much two value are linearly related to each other as well as the scale of these value.\n",
    "\n",
    "$$\n",
    "Cov(f(x), g(y)) = \\mathbb{E}[(f(x)- \\mathbb{E}[f(x)])(g(y)- \\mathbb{E}[g(y)])]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information theory\n",
    "Information theory deals with quantification of how much information is present in a signal. In context of machine learning, information theory we apply information theory to: *characterize probability distributions* and *quantify similarities between probability distributions*. The following are the key information concepts and their application to machine learning.\n",
    "\n",
    "###  Entropy, Cross Entropy and Mutual information\n",
    "\n",
    "Entropy give measure of uncertainty in a random experiment. It help us  quantify the amount of uncertainty in an entire probability distribution. The entropy of a probability distribution is the expected amount of information in an event drawn from that distribution defined as.\n",
    "\n",
    "$$\n",
    "H(X) = -\\mathbb{E}_{x \\sim P}[\\log P(x)] = -\\sum_{i=1}^n P(x_i)l\\log P(x_i)\n",
    "$$\n",
    "\n",
    "Entropy is widely used in model selection based on principle of maximum entropy. On the other hand, cross entropy is used to compare two probability distribution. It tell how similar two distribution are. The cross entropy between two probability distribution $$P$$ and $$Q$$ defined over same set of outcome is given by \n",
    "\n",
    "$$\n",
    "H(P,Q)= -\\sum P(x)\\log Q(x)\n",
    "$$ \n",
    "\n",
    "Cross entropy loss function is widely used in machine learning for classification problem. The mutual information over two random variables help us gain insight about the information that one random variable carries about the other. \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "I(X, Y) &= \\sum P(x, y)\\log \\frac{P(x,y)}{P(x).P(y)}\\\\\n",
    "        &=H(X)- H(X\\mid Y) = H(Y) - H(Y\\mid X)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "From above equation the mutual information  give insight about how far $X$ and $Y$ from being independent from each other. Mutual information can be used in feature selection instead of correlation as it capture both linear and non linear dependency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Kullback-leibler Divergence \n",
    "\n",
    "**Kullback-leibler Divergence** measure how one probability distribution diverge from the other. Given two probability distribution $P(x)$ and $Q(X)$ where the former is the modelled/estimated distribution and the later is the actual/expected distribution. The **KL** divergence is defined as \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "D_{KL}(P||Q) & = \\mathbb{E}_{x \\sim P} [\\log \\frac{P(x)}{Q(x)}]\\\\\n",
    "             & = \\mathbb{E}_{x \\sim P}[\\log P(x)] - \\mathbb{E}_{x \\sim P}[\\log Q(x)]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "For discrete random distribution \n",
    "\n",
    "$$\n",
    "D_{KL}(P||Q) = \\sum_{i} P(x_i)\\log \\frac{P(x_i)}{Q(x_i)}\n",
    "$$\n",
    "\n",
    "And for continuous random variable\n",
    "\n",
    "$$\n",
    "D_{KL}(p||q) = \\int_{x} p(x) \\log \\frac{p(x)}{q(x)}\n",
    "$$\n",
    "\n",
    "KL divergence between $$P$$ and $Q$ tells how much information we lose when trying to approximate data given by $$P$$ with $Q$. It is non-negative $D_{KL}(P\\mid \\mid Q) \\geq 0$ and  $0$ if $P$ and $Q$ are the same (distribution discrete) or equal almost anywhere in the case of continuous distribution. Apart from that KL divergence is not symmetric $D_{KL}(P\\mid \\mid Q) \\neq D_{KL}(P\\mid \\mid Q)$ because of this it is not a true distance measure.\n",
    "\n",
    "**Relation between KL divergence and Cross Entropy**\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "D_{KL}(P||Q) & = \\mathbb{E}_{x \\sim P} [\\log \\frac{P(x)}{Q(x)}]\\\\\n",
    "             & = \\mathbb{E}_{x \\sim P}[\\log P(x)] - \\mathbb{E}_{x \\sim P}[\\log Q(x)]\\\\\n",
    "             & = H(P) - H(P, Q)\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\mathbb{E}_{x \\sim P}[\\log P(x)] = H(P)$$ and $$\\mathbb{E}_{x \\sim P}[\\log Q(x)] = H(P, Q)$. Thus  $H(P,Q) = H(P) - D_{KL}(P||Q)$. This implies that minimizing cross entropy with respect to $Q$ is equivalent to minimizing the KL divergence. KL divergence is used in unsupervised machine learning technique like variational auto-encoder. The KL divergence is also used  as objective function in variational bayesian method to find optimal value for approximating distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning from probabilistic models\n",
    "\n",
    "Given some data $ \\mathbf{x}=[x_1\\ldots x_m]$  that come from some probability density function characterized by\n",
    "an unknown parameter $ \\theta$. How can we find $ \\hat{\\theta}$  that is the best estimator of $ \\theta$. For example suppose we have flipped a particular coin $ 100$  times and landed head $ N_H = 55$  times and tails $ N_T = 45$  times. We are interested to know what is the probability that it will come-up head if we flip it again. In this case the behavior of the coin can be summerized with parameter  $ \\theta$  the probability that a flip land head (H), which in this case is independent and identically distributed Bernoulli distribution. The key question is, how do we find parameter  $ \\hat{\\theta}$  of this distribution that fits the data. This is called parameter estimation, in which three approaches can be used:\n",
    "\n",
    "1. Maximum-Likehood estimation\n",
    "2. Bayesian parameter estimation and \n",
    "3. Maximum a-posterior approximation\n",
    "\n",
    "### Like-hood and log-likehood function\n",
    "\n",
    "Before discussing the above learning approach, let firts define the **like-hood function** $L(\\theta)$ which is the probability of the observed data as function of $\\theta$ given as:\n",
    "\n",
    "$$\n",
    "L(\\theta) = P(x_1,\\ldots x_m; \\theta) = \\prod_i^m P(x_i;\\theta)\n",
    "$$\n",
    "\n",
    "The like-hood function indicates how likely each value of the parameter is to have generated the data. In the case of coin example above, the like-lihood is the probability of particular seqeuence of H and T generated:\n",
    "\n",
    " $$\n",
    " L(\\theta) = \\theta ^{N_H}(1 - \\theta ^{N_T})\n",
    " $$ \n",
    "\n",
    "We also define the **log-likelihood function** $\\mathcal{L}(\\theta)$ which is the log of the likelihood function $L(\\theta)$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}(\\theta) &= \\log L(\\theta) \\\\\n",
    " & = \\log \\prod_i^m P(x_i;\\theta) \\\\\n",
    "  & = \\sum_i^M P(x_i;\\theta)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "For the above coin example the log-likelihood is \n",
    "\n",
    "$$ \n",
    "\\mathcal{L}(\\theta)= N_H\\log\\theta + N_T\\log(1-\\theta)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Maximum-Likelihood Estimation\n",
    "\n",
    "The main objective of maximum likelihood estimation (MLE) is to determine the value of $\\theta$ that is most likely to have generated the vector of observed data, $\\mathbf{x}$ where $\\theta$ is assumed to be  fixed point (point-estimation). MLE achieve this by finding the parameter that maximize the probability of the observed data. The parameter $\\hat{\\theta}$ is selected such that it maximize $  \\mathcal{L}(\\theta)$:\n",
    " \n",
    " $\n",
    "\\hat{\\theta}=\\arg\\max_{\\theta} \\mathcal{L}(\\theta)\n",
    "$\n",
    "\n",
    "For the coin example the MLE is :\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\theta} & = \\frac{\\partial }{\\partial \\theta}(N_H\\log\\theta + N_T\\log(1-\\theta) \\\\\n",
    " &= \\frac{N_H}{\\theta} - \\frac{N_T}{1-\\theta}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Set $\\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\theta} = 0$  and  solve for $\\theta$ we obtain the MLE: \n",
    "\n",
    "$\n",
    "\\hat{\\theta} =\\frac{N_H}{N_H + N_T}\n",
    "$\n",
    "\n",
    "which is simply the fraction of flips that cameup head.\n",
    "\n",
    "Now suppose we are observing power-meta data which can be modelled as gaussian ditribution with mean $$\\mu$$ and standard deviation $\\sigma$. We can use MLE to estimate $\\hat{\\mu}$ and $\\hat{\\sigma}$. The log-likehood for gausian distribution is given as \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}(\\theta) &= \\sum_{i=1}^M \\log \\left[ \\frac{1}{\\sqrt{2} \\pi \\sigma} \\exp \\frac{-(x_i - \\mu)}{2\\sigma ^2}\\right] \\\\\n",
    " & = -\\frac{M}{2}\\log 2\\pi - M\\log \\sigma - \\frac{1}{2\\sigma^2} \\sum_i^M (x_i - \\mu)^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Let find $ \\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\mu} $ and $\\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\sigma} $ and set  equal to zero.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\mu} &=  -\\frac{1}{2\\sigma^2} \\sum_i^M \\frac{\\partial}{\\partial \\mu}(x_i - \\mu)^2 \\\\\n",
    "& = \\sum_i^M (x_i - \\mu) = 0 \\\\\n",
    "&\\Rightarrow \\hat{\\mu} = \\frac{1}{M} \\sum_{i=1}^M x_i\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "which is the mean of the observed values. Similary:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\sigma} &=  \\frac{M}{\\sigma} + \\frac{1}{\\sigma^3}\\sum_i^M (x_i - \\mu)^2 \\\\\n",
    "&\\Rightarrow \\hat{\\sigma} = \\sqrt{\\frac{1}{M} \\sum_{i=1}^M (x_i - \\mu)^2}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In the two examples above  we manged to obtain the exact maximum likelihood solution analytically. But this is not always the case, letâ€™s consider how to compute the maximum likelihood estimate of the parameters of the gamma distribution, whose PDF is defined as:\n",
    "\n",
    "$$\n",
    "P(x) = \\frac{b^a}{\\Gamma(a)}x^{x-1}\\exp(-bx)\n",
    "$$\n",
    "\n",
    "where $\\Gamma (a)$ is the gamma function which is the generalization of the factorial function to continous values given as:\n",
    "\n",
    "$\n",
    "\\Gamma(t) = \\int_0^{-\\infty} x^{t-1}\\exp(-x) \\,dx\n",
    "$\n",
    "\n",
    "The model parameters for gamma distribution is $a$ and $b$ both of which are $\\geq 0$. the log-likelihood is therefore:\n",
    "\n",
    "$\n",
    "\\begin{aligned} \\mathcal{L}( a, b) & = \\sum_{i=1}^M a\\log b -\\log \\Gamma (a) + (a -1) \\log x_i - bx_i \\\\\n",
    " & = Ma\\log b - M \\log \\Gamma (a) + (a - 1) \\sum_{i=1}^M \\log x_i - b \\sum_{i=1}^M x_i\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "To get MLE we need employ gradient descent which consists of computing the derivatives:\n",
    "$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial a}\n",
    "$ and $\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b}\n",
    "$ and then updating; $\n",
    "a_{k+1}= a_k + \\alpha \\frac{\\partial \\mathcal{L}}{\\partial a}\n",
    "$\n",
    "and \n",
    "$\n",
    "b_{k+1}= b_k + \\alpha \\frac{\\partial \\mathcal{L}}{\\partial b}\n",
    "$\n",
    "\n",
    "where $\\alpha$ is the learning rate.\n",
    "\n",
    "\n",
    "### Limitation of MLE\n",
    "\n",
    "Despite the fact that MLE is very powerful technique, it has a pitfall for little training data which can lead into seriously overfit. The most painful issue is when it assign a $0$ probability to items that were never seen in the training data but which still might actually happen. Take an example if we flipped  a coin twice and $N_H = 2$, the MLE of $\\theta$, the probability of H would be $1$. This imply that we are considering it impossible for the coin to come up T. This problem is knowas *data sparsity*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Parameter Estimation\n",
    "\n",
    "Unlike MLE which treat only the observation $\\mathbf{x}$ as random variable and the parameter $\\theta$ as a fixed point, the bayesian approach treat the parameter $\\theta $ as random varibale as well with some known prior distribution. Let define the model for joint distribution $$p(\\theta, \\mathcal{D})$$ over parameter  $\\theta$ and data $\\mathcal{D}$. To further define this joint distribution we aslo need the following two distribution:\n",
    "\n",
    "* A distribution of $P(\\theta)$ knowas **prior distribution** which is the probability of paratemeter $\\theta$ availabe beforehand, and before making any additional observations. It account for everything you believed about the parameter $\\theta$ before observing the data. In practise choose prior that is computational convinient.\n",
    "\n",
    "* The **likelihood** $P(\\mathcal{D}\\mid \\theta)$ which is the probability of data given the parameter like in maximum likelihood.\n",
    "\n",
    "With this two distributions, we can compute the posterior distribution and the posterior predictive distribution. The posterior distribution $P(\\theta \\mid \\mathcal{D})$ which correspond to uncertainty about $\\theta$ after observing the data given by:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(\\theta  \\mid  \\mathcal{D}) &= \\frac{P(\\theta)p(\\mathcal{D}  \\mid  \\theta)}{P(\\mathcal{D})} &= \\frac{P(\\theta)P(\\mathcal{D} \\mid  \\theta)}{ \\displaystyle \\int P(\\theta ^ {\\prime} ) P(\\mathcal{D} \\mid  \\theta ^{\\prime})}\n",
    "\\end{aligned} \n",
    "$$\n",
    "\n",
    "The denominator is usually considered as a normalizing constant and thus the posterior distribution become:\n",
    "\n",
    "$$\n",
    "P(\\theta  \\mid  \\mathcal{D}) \\propto P(\\theta)P(\\mathcal{D}  \\mid \\theta)\n",
    "$$\n",
    "\n",
    "On the other hand the posterior predictive distribution $P(\\mathcal{D}^{\\prime}\\mid)\\mathcal{D}$ is the distribution of future observation given past observation defined by:\n",
    "\n",
    "$$\n",
    "P(\\mathcal{D}^{\\prime} \\mid \\mathcal{D} )= \\int P(\\theta\\mid \\mathcal{D}) P(\\mathcal{D}^{\\prime} \\mid \\theta)\n",
    "$$\n",
    "\n",
    "Generaly the Bayesian approach to parameter estimation works as follows: \n",
    "\n",
    "1. First we need to formulate our knowledge about a situation by defining a distribution model which expresses qualitative aspects of our knowledge about the situation and then specify a prior probability distribution which expresses our subjective\n",
    "beliefs and subjective uncertainty about the unknown parameters, before seeing the data.\n",
    "2. Gather data\n",
    "3. Obtain posterior knowledge that updates our beliefs by computing the posterior probability distribution which estimates the unknown\n",
    "parameters. \n",
    "\n",
    "Let apply the bayesian estimation to the coin example in which we have specified the likelihood equal to $\\theta^{N_H}(1-\\theta)^{N_T}$. We only required to specify the prior in which several approches can be used. One of the approach is relay upon lifetime experince of flipping coins in which most coins tend to be fair which implies $p(\\theta) = 0.5$. We can also use various distribution to specify prior density but in practise a most useful distribution is the **beta distribution** parameterized by $a , b > 0$ and defined as:\n",
    "\n",
    "$$\n",
    "p(\\theta; a, b) = \\frac{\\Gamma (a + b)}{\\Gamma(a) \\Gamma (b)} \\theta ^{a-1}(1-\\theta ^{b - 1})\n",
    "$$\n",
    "\n",
    "From the above eqution it is clear that the first term (with all $\\Gamma$)is just a normalizing constant and thus we can rewrite the beta distribution as:\n",
    "\n",
    "$$\n",
    "p(\\theta; a, b) \\propto \\theta ^{a-1}(1-\\theta) ^{b - 1}\n",
    "$$\n",
    "\n",
    "Note the beta distribution has the following properties\n",
    "\n",
    "* It is centered around $\\frac{a}{a + b}$ and it can be shown that if $\\theta \\sim \\text{Beta}(a,b)$ then $\\mathbb{E}(\\theta)=\\frac{a}{a + b}$.\n",
    "* It becomes more peaked for larger values of $a$ and $b$\n",
    "* It become normal distribution when $a = b = 1$\n",
    "\n",
    "Now let compute the posterior and posterior predictive distribution\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(\\theta | \\mathcal{D}) & \\propto p(\\theta)p(\\mathcal{D} |\\theta) \\\\\n",
    "& \\propto \\theta^{N_H}(1-\\theta)^{N_T}\\theta ^{a-1}(1-\\theta) ^{b - 1} \\\\\n",
    "& = \\theta ^{a-1+N_H}(1-\\theta) ^{b - 1 + N_T}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
