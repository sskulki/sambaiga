{
  
    
        "post0": {
            "title": "Inference in Pyro",
            "content": "import torch import pyro import pyro.distributions as dist from torch.distributions import constraints def model_global_trend(p): slope = pyro.param(&#39;slope&#39;, torch.tensor(0.)) sigma = pyro.param(&#39;sigma&#39;, torch.tensor(1.0), constraints=dist.constraints.positive) intercept = pyro.param(&#39;intercept&#39;, torch.tensor(13.)) for i in range(len(y)): log_p_hat = slope * i + intercept pyro.sample(&#39;y_{}&#39;.format(i), dist.LogNormal(log_p_hat, sigma), obs=p[i]) . Plate statement . From the given model above , pyro.param designate model parameters that we would like to optimize. Observations are denoted by the obs= keyword argument to pyro.sample. This specifies the likelihood function. Instead of log transforming the data, we use a LogNormal distribution. The observations are conditionally independent given the latent random variable slope and intercept. To explicitly mark this in Pyro, plate statement is used to construct conditionally independent sequences of variables. . with pyro.plate(&quot;name&quot;, size, subsample_size, device) as ind: # ...do conditionally independent stuff with ind... . However compared to range() each invocation of plate requires the user to provide a unique name. The plate statement can be used either sequentially as a generator or in parallel as a context manager. Sequential plate is similar to range()in that it generates a sequence of values. . # This version declares sequential independence and subsamples data: for i in plate(&#39;data&#39;, 100, subsample_size=10): if z[i]: # Control flow in this example prevents vectorization. obs = sample(&#39;obs_{}&#39;.format(i), dist.Normal(loc, scale), obs=data[i]) . Vectorized plate is similar to torch.arange() in that it yields an array of indices by which other tensors can be indexed. However, unlike torch.arange() plate also informs inference algorithms that the variables being indexed are conditionally independent. . # This version declares vectorized independence: with plate(&#39;data&#39;): obs = sample(&#39;obs&#39;, dist.Normal(loc, scale), obs=data) . Additionally, plate can take advantage of the conditional independence assumptions by subsampling the indices and informing inference algorithms to scale various computed values. This is typically used to subsample minibatches of data: . with plate(&quot;data&quot;, len(data), subsample_size=100) as ind: batch = data[ind] assert len(batch) == 100 .",
            "url": "https://sambaiga.github.io/sambaiga/jupyter/2020/03/04/ppl-pyro-two.html",
            "relUrl": "/jupyter/2020/03/04/ppl-pyro-two.html",
            "date": " ‚Ä¢ Mar 4, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Probabilistic Programming with Pyro",
            "content": "Intro to Pyro . Pyro is a universal probabilistic programming language (PPL) written in Python and supported by PyTorch on the backend. It enables flexible and expressive deep probabilistic modeling, unifying the best of modern deep learning and Bayesian modeling. . Models and Probability distributions . Models are the basic unit of probabilistic programs in pyro, they represent simplified or abstract descriptions of a process by which data are generated. Models in pyro are expressed as stochastic functions which implies that models can be composed, reused, imported, and serialized just like regular Python callables. Probability distributions (pimitive stochastic functions) are important class of models (stochastic functions) used explicitly to compute the probability of the outputs given the inputs. Pyro uses PyTorch‚Äôs distribution library which contains parameterizable probability distributions and sampling functions. This allows the construction of stochastic computation graphs and stochastic gradient estimators for optimization. Each probability distributions are equipped with several methods such as: . prob(): $ log p( mathbf{x} mid theta ^{*})$ | mean: $ mathbb{E}_{p( mathbf{x} mid theta ^{*})}[ mathbf{x}]$ | sample: $ mathbf{x}^{*} sim {p( mathbf{x} mid theta ^{*})}$ | . You can also create custom distributions using transforms. . Example 1: Let define the unit normal distribution $ mathcal{N}(0,1)$, draw sample $x$ and compute the log probability according to the distribution. . import torch import pyro import pyro.distributions as dist from torch.distributions import constraints import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns pyro.set_rng_seed(101) torch.manual_seed(101) torch.set_printoptions(precision=3) %matplotlib inline . mu = 0 sigma = 1 normal=dist.Normal(mu, sigma) x = normal.rsample() # draw a sample from N(1,1) print(&quot;sample&quot;, x.item()) #To compute the log probability according to the distribution print(&quot;prob&quot;, torch.exp(normal.log_prob(x)).item()) # score the sample from N(1,1) . sample -1.3905061483383179 prob 0.15172401070594788 . Sample and Param statements . Pyro simplifies the process of sampling from distributions with the use of pyro.sample statement. The pyro.sample statement call stochastic functions or models with a unique name as identifier. Pyro‚Äôs backend uses these names to uniquely identify sample statements and change their behavior at runtime depending on how the enclosing stochastic function is being used. Using pyro.sample statement, Pyro can implement various manipulations that underlie inference algorithms. . x = pyro.sample(&quot;name&quot;, fn, obs) &quot;&quot;&quot; name ‚Äì name of sample fn ‚Äì distribution class or function obs ‚Äì observed datum (optional; should only be used in context of inference) optionally specified in kwargs &quot;&quot;&quot; . Example 2: Let sample from previous normal distribution created in example 1. . mu = 0 sigma = 1 x = pyro.sample(&quot;my_sample&quot;, dist.Normal(mu, sigma)) print(x) . tensor(-0.815) . The above code generate a random value and records it in the Pyro runtime. . data=2 x = pyro.sample(&quot;my_sample&quot;, dist.Normal(mu, sigma), obs=data) print(x) . 2 . /opt/miniconda3/lib/python3.7/site-packages/pyro/primitives.py:86: RuntimeWarning: trying to observe a value outside of inference at my_sample RuntimeWarning) . The above code conditions a stochatsic function on observed data. This should run on inference. . Pyro use pyro.param statement to saves the variable as a parameter in the param store. To interact with the param store. The pyro.param statement is used by pyro to declares a learnable parameter. . x = pyro.param(&quot;name&quot;, init_value, constraints) &quot;&quot;&quot; name ‚Äì name of param init_value ‚Äì initial value constraint ‚Äì torch constraint &quot;&quot;&quot; . Example 3: Let create theta parameter . theta = pyro.param(&quot;theta&quot;, torch.tensor(1.0), constraint=dist.constraints.positive) . Simple PPL model . Consider the following Poison Regression model begin{align} y(t) &amp; sim lambda exp(- lambda) lambda &amp; sim exp(c + m(t)) c &amp; sim mathcal{N}(1, 1) m &amp; sim mathcal{N}(0, 1) end{align} . def model(y): slope = pyro.sample(&quot;slope&quot;, dist.Normal(0, 0.1)) intercept = pyro.sample(&quot;intercept&quot;, dist.Normal(0, 1)) for t in range(len(y)): rate = torch.exp(intercept + slope * t) y[t] = pyro.sample(&quot;count_{}&quot;.format(t), dist.Poisson(rate), obs=y[t]) return slope, intercept, y . Given a pyro model. We can . Generate data from model | Learn parameters of the model from data | Use the model to predict future observation. | Generate data from model . Running a Pyro model will generate a sample from the prior. . pyro.set_rng_seed(0) # We pass counts = [None, ..., None] to indicate time duration. true_slope, true_intercept, true_counts = model([None] * 50) fig, ax = plt.subplots(figsize=(6,4)) ax = sns.lineplot(x=np.arange(len(true_counts)),y=[c.item() for c in true_counts]) . Learn parameters of the model from data . To learn model parameters we pass the model to an inference algorithm and let the algorithm guess what the model is doing based on observed data. Inference algorithms in Pyro us arbitrary stochastic functions as approximate posterior distributions. that s. These functions are called guide functions or guides and contains pyro.sample and pyro.param statement. It is a stochastic function that represents a probability distribution over the latent (unobserved) variables. The guide can be arbitrary python code just like the model, but with a few requirements: . All unobserved sample statements that appear in the model appear in the guide. | The guide has the same input signature as the model (i.e. takes the same arguments). | There are no pyro.sample statements with the obs keyword in the guide. These are exclusive to the model. | There are pyro.param statements, which are exclusive to the guide. These provide differentiation for the inputs to the pay_probs sample in the guide vs. the model. | For example if the model contains a random variable z_1 . def model(): pyro.sample(&quot;z_1&quot;, ...) . then the guide needs to have a matching sample statement . def guide(): pyro.sample(&quot;z_1&quot;, ...) . Once a guide has been specified, we can then perform learning and inference which is an optimization problem of maximizing the evidence lower bound (ELBO). The ELBO, is a function of both $ theta$ and $ phi$, defined as an expectation w.r.t. to samples from the guide: . $${ rm ELBO} equiv mathbb{E}_{q_{ phi}({ bf z})} left [ log p_{ theta}({ bf x}, { bf z}) - log q_{ phi}({ bf z}) right]$$The SVI class is unified interface for stochastic variational inference in Pyro. To use this class you need to provide: . the model, | the guide, and an | optimizer which is a wrapper a for a PyTorch optimizer as discusseced in below | . from pyro.infer import SVI, Trace_ELBO svi = SVI(model, guide, optimizer, loss=Trace_ELBO()) . The SVI object provides two methods, step() and evaluate_loss(), . The method step() takes a single gradient step and returns an estimate of the loss (i.e. minus the ELBO). | The method evaluate_loss() returns an estimate of the loss without taking a gradient step. | . Both of these methods accept an optional argument: num_particles, which denotes the number of samples used to compute the loss and gradient. . The module pyro.optim provides support for optimization in Pyro. In particular it provides PyroOptim, which is used to wrap PyTorch optimizers and manage optimizers for dynamically generated parameters. PyroOptim takes two arguments: . a constructor for PyTorch optimizers optim_constructor and | a specification of the optimizer arguments optim_args | . from pyro.optim import Adam adam_params = {&quot;lr&quot;: 0.005, &quot;betas&quot;: (0.95, 0.999)} optimizer = Adam(adam_params) . Thus to learn model parameters we pass the model to an inference algorithm and let the algorithm guess what the model is doing based on observed data (here true_counts). . For the above example we will use Autoguide pyro inference algorithm: . AutoLaplaceApproximation:Laplace approximation (quadratic approximation) approximates the posterior logùëù(ùëß|ùë•) by a multivariate normal distribution in the unconstrained space. | Autodelta: This implementation of AutoGuide uses Delta distributions to construct a MAP guide over the entire latent space. | . from pyro.infer.autoguide import AutoDelta from pyro.infer import SVI, Trace_ELBO from pyro.optim import Adam guide = AutoDelta(model) svi = SVI(model, guide, Adam({&quot;lr&quot;: 0.1}), Trace_ELBO()) for i in range(101): loss = svi.step(true_counts) # true_counts is passed as argument to model() if i % 10 == 0: print(&quot;loss = {}&quot;.format(loss)) . loss = 87295.88946688175 loss = 64525.8595520854 loss = 80838.8460238576 loss = 33014.93229973316 loss = 13704.865498423576 loss = 6232.828522503376 loss = 2017.9879159331322 loss = 631.3558134436607 loss = 170.10323333740234 loss = 198.57187271118164 loss = 207.4590385556221 . print(&quot;true_slope = {}&quot;.format(true_slope)) print(&quot;true_intercept = {}&quot;.format(true_intercept)) guess = guide() print(&quot;guess = {}&quot;.format(guess)) . true_slope = 0.15409961342811584 true_intercept = -0.293428897857666 guess = {&#39;slope&#39;: tensor(0.147, grad_fn=&lt;ExpandBackward&gt;), &#39;intercept&#39;: tensor(-0.054, grad_fn=&lt;ExpandBackward&gt;)} . Use model to predict future observation . A third way to use a Pyro model is to predict new observed data by guiding the model. This uses two of Pyro&#39;s effects: . trace records guesses made by the guide, and | replay conditions the model on those guesses, allowing the model to generate conditional samples. | . Traces are directed graphs whose nodes represent primitive calls or input/output, and whose edges represent conditional dependence relationships between those primitive calls. It return a handler that records the inputs and outputs of primitive calls and their dependencies. . We can record its execution using trace and use the resulting data structure to compute the log-joint probability of all of the sample sites in the execution or extract all parameters. . trace = pyro.poutine.trace(model).get_trace([]) pprint({ name: { &#39;value&#39;: props[&#39;value&#39;], &#39;prob&#39;: props[&#39;fn&#39;].log_prob(props[&#39;value&#39;]).exp() } for (name, props) in trace.nodes.items() if props[&#39;type&#39;] == &#39;sample&#39; }) . {&#39;intercept&#39;: {&#39;prob&#39;: tensor(0.250), &#39;value&#39;: tensor(-0.966)}, &#39;slope&#39;: {&#39;prob&#39;: tensor(2.818), &#39;value&#39;: tensor(0.083)}} . print(trace.log_prob_sum().exp()) . tensor(0.705) . Here, the trace feature will collect values every time they are sampled with sample and store them with the corresponding string name (that‚Äôs why we give each sample a name). With a little cleanup, we can print out the value and probability of each random variable‚Äôs value, along with the joint probability of the entire trace. . Replay return a callable that runs the original, reusing the values at sites in trace at those sites in the new trace. makes sample statements behave as if they had sampled the values at the corresponding sites in the trace . from pyro import poutine def forecast(forecast_steps=10): counts = true_counts + [None] * forecast_steps # observed data + blanks to fill in guide_trace = poutine.trace(guide).get_trace(counts) _, _, counts = poutine.replay(model, guide_trace)(counts) return counts . We can now call forecast() multiple times to generate samples. . for _ in range(1): full_counts = forecast(10) forecast_counts = full_counts[len(true_counts):] plt.plot([c.item() for c in full_counts], &quot;r&quot;, label=None if _ else &quot;forecast&quot;, alpha=0.3) plt.plot([c.item() for c in true_counts], &quot;k-&quot;, label=&quot;truth&quot;) plt.legend(); . References . Pyro-ducomentation | PPL models for timeseries forecasting |",
            "url": "https://sambaiga.github.io/sambaiga/jupyter/2020/03/01/ppl-pyro-intro.html",
            "relUrl": "/jupyter/2020/03/01/ppl-pyro-intro.html",
            "date": " ‚Ä¢ Mar 1, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Title",
            "content": "Pytorch for Deeplearning Research . **Anthony Faustine (sambaiga@gmail.com)** import torch import numpy as np import random import matplotlib.pyplot as plt %matplotlib inline . def set_seed(seed = 4783957): print(&quot;set seed&quot;) np.random.seed(seed) random.seed(seed) if torch.cuda.is_available(): torch.cuda.manual_seed(seed) torch.manual_seed(seed) . set_seed() # for reproducibility . set seed . 6. Introduction . 6. 1 What is Pytorch . PyTorch is a python package that provides two high-level features: Tensor computation (like numpy) with strong GPU acceleration and Deep Neural Networks built on a tape-based autograd system [1](#myfootnote1). These features enable pytorch to act as a replacement for numpy to use the power of GPUs and deep learning research platform that provides maximum flexibility and speed . Unlike Tensorfolow, PyTorch supports creation of dynamic computation graphs (DCG), whereas Tensorflow use a static computation graph (SCG). For a clear and systematically comparison between PyTorch and TensorFlow you may refer to this blog post. . This notebook require PyTorch 0.4.0 or later. You can check the version number of the currently installed pytorch package with: python print(torch.__version__). . print(torch.__version__) . 1.1.0 . 6. 2. Basic Pytorch Operations . 6.2.1 Pytorch Tensors . The main building block of the PyTorch is the tensors. So what is tensor? . Tensor is a multi-dimensional matrix containing elements of a single data type. They are very similar to the NumPy array. However, unlike numpy array, pytorch tensor can utilize GPU. . A tensor can be constructed from a Python list or sequence with the torch.tensor() function. . #Create a torch.Tensor object with the given data. It is a 1D vector data = [1., 2., 3.] V = torch.tensor(data) print(V) . tensor([1., 2., 3.]) . # Creates a matrix data = [[1., 2., 3.], [4., 5., 6]] M = torch.tensor(data) print (M) . tensor([[1., 2., 3.], [4., 5., 6.]]) . # Create a 3D tensor of size 2x2x2. data = [[[1.,2.], [3.,4.]], [[5.,6.], [7.,8.]]] T = torch.tensor(data) print (T) . tensor([[[1., 2.], [3., 4.]], [[5., 6.], [7., 8.]]]) . Vectors and matrices are special cases of torch.Tensors, where their dimension is 1 and 2 respectively . You can create a tensor with random data and the supplied dimensionality with torch.randn() . x = torch.randn(2, 4) print(x) . y = torch.randn(2, 2, 4) print(y) . You can also use special tensors line ones and zeros . torch.ones(2, 3) . torch.zeros(3, 5) . torch.empty(2, 3) . tensor([[6.3588e+19, 4.5755e-41, 7.0981e-03], [3.0740e-41, 4.4842e-44, 0.0000e+00]]) . To get size of tensor you can use .size(), it also possible to use .shape . x = torch.rand(3, 2) x . x.size() . x.shape . To returns the value of this tensor as a standard Python number use .item(). This only works for tensors with one element. . x = torch.rand(3) . print(x[0]) . tensor(0.1920) . print(x[0].item()) . 0.19197648763656616 . Numpy Bridge . You can easily convernt pytorh tensor into numpy array and viceversa. To create a tensor from a Numpy array, use torch.from_numpy() or torch.Tensor(). To convert a tensor to a Numpy array, use the .numpy() method. . import numpy as np numpy_tensor = np.random.randn(3, 4) print(numpy_tensor) . [[-0.03320803 1.02821944 -0.3637978 -0.88306562] [-1.38147289 -1.00781227 0.76164331 -0.41654529] [ 1.46609225 -0.27520039 0.00516482 -0.75045402]] . # convert numpy array to pytorch array pytorch_tensor = torch.Tensor(numpy_tensor) print(pytorch_tensor) . tensor([[-0.0332, 1.0282, -0.3638, -0.8831], [-1.3815, -1.0078, 0.7616, -0.4165], [ 1.4661, -0.2752, 0.0052, -0.7505]]) . # use from_numpy pytorch_tensor = torch.from_numpy(numpy_tensor) print(pytorch_tensor) . tensor([[-0.0332, 1.0282, -0.3638, -0.8831], [-1.3815, -1.0078, 0.7616, -0.4165], [ 1.4661, -0.2752, 0.0052, -0.7505]], dtype=torch.float64) . # convert torch tensor to numpy representation pytorch_tensor.numpy() . array([[-0.03320803, 1.02821944, -0.3637978 , -0.88306562], [-1.38147289, -1.00781227, 0.76164331, -0.41654529], [ 1.46609225, -0.27520039, 0.00516482, -0.75045402]]) . 6.2.3 Operations with Tensors . You can operate on tensors in the ways you would expect. . x = torch.Tensor([ 1., 2., 3. ]) y = torch.Tensor([ 4., 5., 6. ]) z = x + y print (z) . # You can also use z = torch.add(x, y) print(z) . For more and compresnive list on pytorch operations follow pytorch documentation . Reshaping Tensors . The .view() method provide a function to reshape a tensor. This method receives heavy use, because many neural network components expect their inputs to have a certain shape. Often you will need to reshape before passing your data to the component. . x = torch.randn(1, 3, 4) x . tensor([[[-0.3049, -0.6309, -1.3195, 1.1236], [ 2.0500, -0.0787, -0.1842, 0.5976], [-2.1318, 2.5275, -0.5717, -0.5014]]]) . # Reshape to 1 rows, 12 columns x.view(1, 12) . tensor([[-0.3049, -0.6309, -1.3195, 1.1236, 2.0500, -0.0787, -0.1842, 0.5976, -2.1318, 2.5275, -0.5717, -0.5014]]) . # Reshape to 1x6x2 x.view(1, 6, 2) . tensor([[[-0.3049, -0.6309], [-1.3195, 1.1236], [ 2.0500, -0.0787], [-0.1842, 0.5976], [-2.1318, 2.5275], [-0.5717, -0.5014]]]) . 6.3 GPU support . Pytorch has GPU support that greatly speed up training of deep learning models by running the matrix operations on a GPU with CUDA. GPU support is implemented in torch.cuda. This package adds support for CUDA tensor types, that implement the same function as CPU tensors, but they utilize GPUs for computation. The new API (v0.4.0) lets us define it in a nice way. . You can use is_available() to determine if your system supports CUDA . if torch.cuda.is_available(): print(&quot;CUDA supported&quot;) else: print(&quot;No cuda support&quot;) . No cuda support . 6.3.1 Move tensors between CPU and GPU . A torch.device contains a device type (&#39;cpu&#39; or &#39;cuda&#39;) and optional device ordinal (id) for the device type. It can be initilized with torch.device(&#39;{device_type}&#39;). . The device attribute of a Tensor gives the torch.device for all Tensors (get_device only works for CUDA tensors) | The to method of Tensors and Modules can be used to easily move objects to different devices (instead of having to call cpu() or cuda() based on the context) | . # at beginning of the script device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) print(&quot;Device type set:&quot;, &quot;GPU&quot; if device.type == &quot;cuda&quot; else &quot;CPU&quot;) . Device type set: CPU . Tensors can be moved onto any device using the .to method. . #convernt tensor into device x = x.to(device) . NOTE: . You can access use PyTorch with GPU in Google Colab. Details on how to get started can be found here . 6.4 Autograd and Variables . Autograd provide a mechanism to compute error gradients and back-propagated through the computational graph. It is a define-by-run framework, which means that your backprop is defined by how your code is run, and that every single iteration can be different. . torch.Tensor is the central class of the package. If you set its attribute .requires_grad as True, it starts to track all operations on it. When you finish your computation you can call .backward() and have all the gradients computed automatically. . To stop a tensor from tracking history, you can call .detach() to detach it from the computation history, and to prevent future computation from being tracked. . To prevent tracking history (and using memory), you can also wrap the code block in with torch.no_grad():. This is helpful when evaluating a model because the model may have trainable parameters with requires_grad=True, but for which we don‚Äôt need the gradients. . Every tensor instance has two attributes: .data that contain initial tensor itself and .grad that will contain gradients for the corresponding tensor. . NOTE: . Computation graph is simply a specification of how your data is combined to give you the output. Since the graph totally specifies what parameters were involved with which operations, it contains enough information to compute derivatives. . For example: if we have $y = wx + b$ it clear that $ frac{ partial y}{ partial x} =w$, $ frac{ partial y}{ partial b} = 1$ and $ frac{ partial y}{ partial w} = x$ . To compute the derivatives, you can call .backward() on a Variable. If Variable is a scalar (i.e. it holds a one element tensor), you don‚Äôt need to specify any arguments to backward(), however if it has more elements, you need to specify a grad_output argument that is a tensor of matching shape. . Example 1: . x = torch.ones(2, 2, requires_grad=True) . # Create tensors x = torch.tensor(1.0, requires_grad=True) w = torch.tensor(2.0, requires_grad=True) b = torch.tensor(3.0, requires_grad=True) . # Build a computational graph. y = w * x + b # y = 2 * x + 3 . # Compute gradients. y.backward() # Print out the gradients. print(x.grad) # x.grad = 2 print(w.grad) # w.grad = 1 print(b.grad) # b.grad = 1 . Example 2: . # Create tensors variables. x = torch.ones((1, 1), requires_grad=True) # perform operations y = x + 2 z = y * y * 3 # find gradient z.backward() #print gradient print(x.grad) . The gradient of x is equal to 18. This is equivalent to: $$ z = 3y^2 text{ where } y = x + 2 Rightarrow z = 3(x + 2)^2 $$ . Thus: $$ frac{dz}{dx} = 6(x +2) = 6(1+2) = 18$$ . 6.5 Deep Learning Building Blocks . Deep learning consists of composing linearities with non-linearities modules. The introduction of non-linearities allows for powerful models. Given linear and non-liear module how to define objective function and train deep learninh model in pytorch. . Neural networks can be constructed using the torch.nn package. It provides pretty much all neural network related functionalities such as : . Linear layers - nn.Linear, nn.Bilinear | Convolution Layers - nn.Conv1d, nn.Conv2d, nn.Conv3d, nn.ConvTranspose2d | Nonlinearities - nn.Sigmoid, nn.Tanh, nn.ReLU, nn.LeakyReLU | Pooling Layers - nn.MaxPool1d, nn.AveragePool2d | Recurrent Networks - nn.LSTM, nn.GRU | Normalization - nn.BatchNorm2d | Dropout - nn.Dropout, nn.Dropout2d | Embedding - nn.Embedding | Loss Functions - nn.MSELoss, nn.CrossEntropyLoss, nn.NLLLoss | . Using the above torch.nn classes requires defining an instance of the class and then running inputs through the instance. . Pytorch provide the functional API thta allows users to use these classes in a functional way. Such as . import torch.nn.functional as F . Linear layers - F.linear(input=x, weight=W, bias=b) | Convolution Layers - F.conv2d(input=x, weight=W, bias=b, stride=1, padding=0, dilation=1, groups=1) | Nonlinearities - F.sigmoid(x), F.tanh(x), F.relu(x), F.softmax(x) | Dropout - F.dropout(x, p=0.5, training=True) | . import torch.nn as nn import torch.nn.functional as F . Linear function (Affine Maps) . This is the core building block of deep learning defined is a function: $$ f(x) = mathbf{wx + b}$$ for a matrix $ mathbf{w} $ and vectors $ mathbf{x,b}$. Linear function is implemented in: torch.nn . torch.nn.Linear(in_features, out_features, bias=True) . Note: pytorch maps the rows of the input instead of the columns . lin = nn.Linear(1, 1, bias=True) x = torch.Tensor(np.arange(-50, 50).reshape(-1,1)) y = lin(x) #print(y) . plt.plot(x.data.numpy(), y.data.numpy(), label=&quot;linear&quot;) plt.title(&quot;Linear Activation&quot;) plt.xlabel(&quot;$x$&quot;) plt.ylabel(&quot;$y$&quot;) plt.legend(); . Non-Linearities Function (Activation Function) . Most used non-linear functions are: sigmoid, tanh and relu function. . ## sigmoid y = F.sigmoid(x) plt.plot(x.data.numpy(), y.data.numpy(), label=&quot;sigmoid&quot;) plt.title(&quot;Sigmoid Activation&quot;) plt.xlabel(&quot;$x$&quot;) plt.ylabel(&quot;$y$&quot;) plt.legend(); . ## Relu y = F.relu(x) plt.plot(x.data.numpy(), y.data.numpy(), label=&quot;ReLU&quot;) plt.title(&quot;ReLU Activation&quot;) plt.xlabel(&quot;$x$&quot;) plt.ylabel(&quot;$y$&quot;) plt.legend(); . ## Relu y = F.tanh(x) plt.plot(x.data.numpy(), y.data.numpy(), label=&quot;Tanh&quot;) plt.title(&quot;Tanh Activation&quot;) plt.xlabel(&quot;$x$&quot;) plt.ylabel(&quot;$y$&quot;) plt.legend(); . Other pytorch modules for defining neural networks . torch.optim: provides implementations of standard stochastic optimization techniques. . torch.distributions: contains parameterizable probability distributions and sampling functions. . 6.6. Creating a neural network . To create a neural network in PyTorch, we use nn.Module base class with Python class inheritance which allows us to use all of the functionality of the nn.Module base class. . class Model(torch.nn.Module): def __init__(self, nb_feature, nb_output): &quot;&quot;&quot; In the constructor we instantiate two nn.Linear module &quot;&quot;&quot; super(Model, self).__init__() self.fc1 = torch.nn.Linear(nb_feature, hidden_size) self.fc2 = torch.nn.Linear(hidden_size, nb_output) def forward(self, x): &quot;&quot;&quot; In the forward function we accept a Variable of input data and we must return a Variable of output data. We can use Modules defined in the constructor as well as arbitrary operators on Variables. &quot;&quot;&quot; x = self.fc1(x) x = F.relu(x) x = self.fc2(x) return x . In the class definition, you can see the inheritance of the base class torch.nn.Module. | Then, in the first line of the class initialization (def init(self):) we have the required Python super() function, which creates an instance of the base torch.nn.Module class. | The next line define a linear object defined by torch.nn.Linear, with the first argument in the definition being the number of input feature and the next argument being the number of output. | After that we need to define how data flows through out network. This can be doe using forward() method in which we supply the input data x as the primary argument. | . PyTorch offers an alternative easier and more convenient way of creating neural networ using torch.nn.Sequential class. You can also define your own layers and add them to the Sequential chain. . nb_feature = 2 hidden_size = 10 nb_output = 1 model_type_2 = torch.nn.Sequential(torch.nn.Linear(nb_feature, hidden_size), torch.nn.ReLU(), torch.nn.Linear(hidden_size, nb_output) ) . The next step is to create an instance of this network architecture and assign this instance to cuda() method if available. Suppose we have the following data. . # Create tensors. x = torch.randn(10, 8) y = torch.randn(10, 1) . model = Model(8, 1) #move to device model = model.to(device) x = x.to(device) y = y.to(device) . We can check the instance of our model: . print(model) . Training the network . To train this model we need to setup an optimizer and a loss criterion: . criterion = torch.nn.BCEWithLogitsLoss() optimizer = torch.optim.SGD(model.parameters(), lr=0.001) . In the first line, we create a stochastic gradient descent optimizer, and we specify the learning rate and supply the model parameters using model.parameters() method of the base torch.nn.Module class that we inherit. | Next, we set our loss criterion to be the MSE loss. For details on different loss function you may refer to pytorch documentation | . In the training process: . First we run optimizer.zero_grad() ‚Äì this zeroes / resets all the gradients in the model, so that it is ready to go for the next back propagation pass. In other libraries this is performed implicitly, but in PyTorch you have to remember to do it explicitly. | Then we we pass the input data into the model pred = model(x) ‚Äì this will call the forward() method in our model class. | After that we get the MSE loss between the output of our network and the target data as loss = criterion(y_pred, y_data). | . optimizer.zero_grad() pred = model(x) loss = criterion(pred, y) print(&#39;loss: &#39;, loss.item()) . Then we runs a back-propagation operation from the loss Variable backwards through the network using loss.backward()* | Finaly we tell PyTorch to execute a gradient descent step based on the gradients calculated during the .backward() operation using optimizer.step(). | . loss.backward() optimizer.step() . 6.7 Data loaders . PyTorch provides two classess the Dataset class and the Dataloader class that can be used to to feed training data into the network. . Dataset class is used to provide an interface for accessing all the training or testing samples in your dataset. To achieve this, you have to implement two method, __getitem__ and __len__ so that each training sample can be accessed by its index. . from torch.utils.data import Dataset, DataLoader class customDataset(Dataset): &quot;&quot;&quot; custom dataset.&quot;&quot;&quot; # Initialize your data, download, etc. def __init__(self, x, y): x = (x - x.mean(axis=0))/(x.std(axis=0)) self.len = x.shape[0] self.x = torch.from_numpy(x).float() self.y = torch.from_numpy(y).float() def __getitem__(self, index): return self.x[index], self.y[index] def __len__(self): return self.len . ## Let us prepare data and define the dataset class import pandas as pd df = pd.read_csv(&quot;../data/pima/diabetes.csv&quot;) features = [&#39;Pregnancies&#39;, &#39;Glucose&#39;, &#39;BloodPressure&#39;, &#39;SkinThickness&#39;, &#39;Insulin&#39;, &#39;BMI&#39;, &#39;DiabetesPedigreeFunction&#39;, &#39;Age&#39;] target = [&#39;Outcome&#39;] inputs = df[features].as_matrix() targets = df[target].as_matrix() . dataset = customDataset(x=inputs, y=targets) #print length of the datasets print(dataset.len) . The Dataloader class accept a dataset and other parameters such as batch size etc to load the data and so Then we can iterate over the Dataloader to get batches of training data and train your models. This class provides several important functionality for building deep learning models such as batching, shuffling, multiprocess data loading, etc . data_loader = DataLoader(dataset=dataset, batch_size=32, shuffle=True) . To access data in data loader . X_data, y_data=next(iter(data_loader)) X_data . To iterate through our data we use for loop as follows . # for training purpose will for i, (x_data, y_data) in enumerate(data_loader, 0): if i ==2: print(x_data) print(y_data) break . def train(model, optimizer, loss_fn, device, data_loader, num_epochs, print_every=2): total_loss = [] model.to(device) loss_fn.to(device) model.train() print(&quot;Start training&quot;) for epoch in range(num_epochs): training_loss = [] for i, (inputs, targets) in enumerate(data_loader): inputs, targets = inputs.to(device), targets.to(device) optimizer.zero_grad() pred = model(inputs) # Calculate Loss: loss = loss_fn(pred, targets) training_loss.append(loss.item()) loss.backward() optimizer.step() total_loss.append(np.mean(training_loss)) if epoch % print_every == 0: print(&#39;Train Epoch: {} [{}/{} ({:.0f}%)] Loss: {:.6f}&#39;.format( epoch+1, i * len(inputs), len(data_loader.dataset), 100. * i / len(data_loader), np.mean(training_loss))) return total_loss . train_loss = train(model, optimizer, criterion, device, data_loader, 1000, 50) . plt.plot(train_loss) plt.title(&quot;Training loss&quot;) plt.xlabel(&quot;iterations&quot;) plt.ylabel(&quot;Loss&quot;) . References: . Adventures in machine learning | DeepLearningZeroToAll | MILA welocome tutorial | PyTorch With Baby Steps: From y = x To Training A Convnet | How to Use Your Own Custom Dataset for Classification in PyTorch | . 1: http://pytorch.org/about/ .",
            "url": "https://sambaiga.github.io/sambaiga/2019/12/05/pytorch-basics.html",
            "relUrl": "/2019/12/05/pytorch-basics.html",
            "date": " ‚Ä¢ Dec 5, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": ". Anthony Faustine is a Data Scientist at CeADAR (UCD), Dublin, Ireland with over four years of successful experience in data analytics and Artificial Intelligence techniques for multiple applications. He effectively researches techniques for novel approaches to problems and develops prototypes to assess their viability. Although Anthony is a person who takes the initiative, he has a strong team-work spirit with experience of working in a highly international environment. . At CeADER, Anthony is devising and implementing data analytics/AI technical solutions for multiple application domains. He is also involved in the research and development of the applicability of Artificial Intelligence for Earth Observation (AI4EO). . Mr Faustine, received the B.sc. Degree in Electronics Science and Communication from the University of Dar es Salaam, Tanzania, and the M.sc. Degree in Telecommunications Engineering from the University of Dodoma, Tanzania, in 2010. From 2010 to 2017, he worked as an assistant lecturer at the University of Dodoma, Tanzania, where he was involved in several research projects within the context of ICT4D. . In 2017, Anthony joined IDLab, imec research group of the University of Ghent, in Belgium as a Ph.D. Machine learning researcher advised by Tom Dhaene and Dirk Deschrijver. His research focused on machine learning techniques applied to energy smart-meter data. He develops methods to identify active appliances and extract their corresponding power consumption from aggregate power (energy-disaggregation) in residential and industrial buildings. . His research interests lie in the intersections between Computational Sustainability and Artificial Intelligence. He works towards bridging the gap between laboratory and real-world applicability of machine learning for sustainable development. .",
          "url": "https://sambaiga.github.io/sambaiga/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}