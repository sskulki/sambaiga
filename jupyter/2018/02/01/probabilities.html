<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <title>Basics of Probability and Information Theory | 
  sambaiga
</title>
  

  
  <meta name="description" content="
  
">
  <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Basics of Probability and Information Theory | sambaiga</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Basics of Probability and Information Theory" />
<meta name="author" content="Anthony Faustine" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The post introduce the basics principle of probability and information theory and their application to machine learning." />
<meta property="og:description" content="The post introduce the basics principle of probability and information theory and their application to machine learning." />
<link rel="canonical" href="https://sambaiga.github.io/sambaiga/jupyter/2018/02/01/probabilities.html" />
<meta property="og:url" content="https://sambaiga.github.io/sambaiga/jupyter/2018/02/01/probabilities.html" />
<meta property="og:site_name" content="sambaiga" />
<meta property="og:image" content="https://sambaiga.github.io/sambaiga/images/probabilities.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-02-01T00:00:00-06:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Anthony Faustine"},"description":"The post introduce the basics principle of probability and information theory and their application to machine learning.","mainEntityOfPage":{"@type":"WebPage","@id":"https://sambaiga.github.io/sambaiga/jupyter/2018/02/01/probabilities.html"},"@type":"BlogPosting","url":"https://sambaiga.github.io/sambaiga/jupyter/2018/02/01/probabilities.html","headline":"Basics of Probability and Information Theory","dateModified":"2018-02-01T00:00:00-06:00","datePublished":"2018-02-01T00:00:00-06:00","image":"https://sambaiga.github.io/sambaiga/images/probabilities.jpg","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/sambaiga/assets/css/style.css">
  <link rel="stylesheet" href="/sambaiga/assets/css/main.css">
  <link rel="canonical" href="https://sambaiga.github.io/sambaiga/jupyter/2018/02/01/probabilities.html"><link type="application/atom+xml" rel="alternate" href="https://sambaiga.github.io/sambaiga/feed.xml" title="sambaiga" />

  

  

  

  <link rel="shortcut icon" type="image/x-icon" href="/sambaiga/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Basics of Probability and Information Theory | sambaiga</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Basics of Probability and Information Theory" />
<meta name="author" content="Anthony Faustine" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The post introduce the basics principle of probability and information theory and their application to machine learning." />
<meta property="og:description" content="The post introduce the basics principle of probability and information theory and their application to machine learning." />
<link rel="canonical" href="https://sambaiga.github.io/sambaiga/jupyter/2018/02/01/probabilities.html" />
<meta property="og:url" content="https://sambaiga.github.io/sambaiga/jupyter/2018/02/01/probabilities.html" />
<meta property="og:site_name" content="sambaiga" />
<meta property="og:image" content="https://sambaiga.github.io/sambaiga/images/probabilities.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-02-01T00:00:00-06:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Anthony Faustine"},"description":"The post introduce the basics principle of probability and information theory and their application to machine learning.","mainEntityOfPage":{"@type":"WebPage","@id":"https://sambaiga.github.io/sambaiga/jupyter/2018/02/01/probabilities.html"},"@type":"BlogPosting","url":"https://sambaiga.github.io/sambaiga/jupyter/2018/02/01/probabilities.html","headline":"Basics of Probability and Information Theory","dateModified":"2018-02-01T00:00:00-06:00","datePublished":"2018-02-01T00:00:00-06:00","image":"https://sambaiga.github.io/sambaiga/images/probabilities.jpg","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://sambaiga.github.io/sambaiga/feed.xml" title="sambaiga" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    // remove paragraph tags in rendered toc (happens from notebooks)
    var toctags = document.querySelectorAll(".toc-entry")
    toctags.forEach(e => (e.firstElementChild.innerText = e.firstElementChild.innerText.replace('¶', '')))
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/sambaiga/">sambaiga</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/sambaiga/about/">About</a><a class="page-link" href="/sambaiga/resources.html">Resources</a><a class="page-link" href="/sambaiga/talks/">Talk</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <div class="wrapper">
    <h1 class="post-title p-name" itemprop="name headline">Basics of Probability and Information Theory</h1><p class="page-description">The post introduce the basics principle of probability and information theory and their application to machine learning.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2018-02-01T00:00:00-06:00" itemprop="datePublished">
        Feb 1, 2018
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      7 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/sambaiga/categories/#jupyter">jupyter</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/sambaiga/sambaiga/tree/master/_notebooks/2018-02-01-probabilities.ipynb" role="button">
<img class="notebook-badge-image" src="/sambaiga/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div><div class="px-2">
    <a href="https://colab.research.google.com/github/sambaiga/sambaiga/blob/master/_notebooks/2018-02-01-probabilities.ipynb">
        <img class="notebook-badge-image" src="/sambaiga/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </div>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#Introduction">Introduction </a></li>
<li class="toc-entry toc-h2"><a href="#Probability-Theory">Probability Theory </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Probability-and-Probability-distribution">Probability and Probability distribution </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Information-theory">Information theory </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Entropy,-Cross-Entropy-and-Mutual-information">Entropy, Cross Entropy and Mutual information </a></li>
<li class="toc-entry toc-h3"><a href="#Kullback-leibler-Divergence">Kullback-leibler Divergence </a></li>
</ul>
</li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2018-02-01-probabilities.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Introduction">
<a class="anchor" href="#Introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction<a class="anchor-link" href="#Introduction"> </a>
</h2>
<p>Probability and Information theory are important field that has made significant contribution to deep learning and AI. Probability theory allows us to make <em>uncertain statements</em> and to <em>reason</em> in the presence of <em>uncertainty</em> where information theory enables us to <em>quantify</em> the amount of <em>uncertainty</em> in a probability distribution.</p>
<h2 id="Probability-Theory">
<a class="anchor" href="#Probability-Theory" aria-hidden="true"><span class="octicon octicon-link"></span></a>Probability Theory<a class="anchor-link" href="#Probability-Theory"> </a>
</h2>
<p>Probability is a mathematical framework for representing uncertainty. It is very applicable in Machine learning and Artificial Intelligence as it allows to make <em>uncertain statements</em> and  <em>reason</em> in the presence of <em>uncertainty</em>. Probability theory allow us to design ML algorithms that take into consideration  of <em>uncertain</em> and sometimes <em>stochastic</em>  quantities. It further tell us tell us how ML systems should <em>reason</em> in the presence of uncertainty. This  is necessary because most things in the world  are uncertain, and thus  ML systems should reason using probabilistic rules. Probability theory can also be used to analyse the behaviour of  ML algorithms probabilistically. Consider evaluating ML classification algorithm using  accuracy metric which is  the probability that the model will give a correct prediction  given an example.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Probability-and-Probability-distribution">
<a class="anchor" href="#Probability-and-Probability-distribution" aria-hidden="true"><span class="octicon octicon-link"></span></a>Probability and Probability distribution<a class="anchor-link" href="#Probability-and-Probability-distribution"> </a>
</h3>
<p><strong>Probability</strong> is a measure of the likelihood that an event will occur in a random experiment. It is quantified as number between 0 and 1. The mathematical function that maps all possible outcome of a random experiment with its associated probability it is called <strong>probability distribution</strong>. It describe how likely a random variable or set of random variable is to take on each of its possible state. The probability distribution for discrete random variable is called probability mass function (PMF) which measures the probability $X$ takes on the value $x$, denoted denoted as $P(X=x)$.
To be PMF on random variable $X$  a function $P(X)$ must satisfy:</p>
<ul>
<li>Domain of $P$ equal to all possible states of $X$</li>
<li>$\forall x \in X, 0\leq P(X=x) \leq 1$</li>
<li>$\sum_{x \in X} P(x) =1$</li>
</ul>
<p>Popular and useful PMF includes poison, binomial, bernouli, and uniform. Let consider a poison  distribution defined as:</p>
$$
P(X=x) = \frac{\lambda ^x e^{ -\lambda}}{x!}
$$<p>$\lambda &gt;0$ is called a parameter of the distribution, and it controls the distribution's shape. By increasing $\lambda$ , we add more probability to larger values, and conversely by decreasing $\lambda$  we add more probability to smaller values as shown in figure below.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Instead of a PMF, a continuous random variable has a probability density function (pdf) denoted as $f_X(x)$. An example of continuous random variable is a random variable with exponential density. 
$$
f_X(x\mid \lambda) = \lambda ^x e^{ -\lambda} \text{,  } x \geq 0
$$</p>
<p>To be a probability density function $p(x)$ must satisfy</p>
<ul>
<li>The domain of $p$ must be the set of all possible state</li>
<li>$\forall x \in X, f_X(x) \geq 0$</li>
<li>$\int_{x \in X} f_X(x)dx =1$</li>
</ul>
<p>The pdf does not give the probability of a specific state directly. The probability that $x$ is between two point $a, b$ is</p>
<p>$\int_{a}^b f_X(x)dx$</p>
<p>The probability of intersection of two or more random variables is called <em>joint probability</em> denoted  as $
P(X, Y)
$</p>
<p>Suppose we have two random variable $X$ and $Y$ and we know the joint PMF or pdf distribution between these variable. The PMF or pdf  corresponding to a single variable is called <em>marginal probability distribution</em> defined as
$$
P(x) = \sum_{y\in Y} P(x, y)
$$</p>
<p>for discrete random variable and 
$$
p(x) = \int p(x)dy
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Marginalization allows us to get the distribution of variable $$X$$ ignoring variable $Y$ from the joint distribution $P(X,Y)$. The probability that some event will occur given we know other events is called condition probability denoted as $P(X\mid Y)$. The  marginal, joint and conditional probability are linked by the following rule 
$
P(X|Y) = \frac{P(X, Y)}{P(Y)}
$</p>
<p><strong>Independence, Conditional Independence and Chain Rule</strong></p>
<p>Two random variables are said to be independent of each other if the probability that one random variables occur in no way affect the probability of the other random variable occurring. $X$ and $Y$ are said to be independent if $P(X,Y) = P(X)\cdot P(Y)$
On the other hand two random variable $X$ and $Y$ are conditionally independent given an event $Z$ with $P(Z)&gt;0$ if</p>
$$
P(X,Y\mid Z) = P(X\mid Y)\cdot P(Y\mid Z)
$$<p>The good example of conditional independence can be found on this <a href="">link</a>. Any joint probability distribution over many random variables may be decomposed into conditional distributions using <em>chain rule</em> as follows:</p>
$$
P(X_1,X_2, \ldots, X_n ) = P(X_1)\prod_{i=2}^n P(X_i\mid X_i, \ldots X_{i-1})
$$<p><strong>Expectation, Variance and Covariance</strong></p>
<p>Expected value of some function $f(x)$ with respect to a probability distribution $P(X)$ is the average or mean value that $f(x)$ takes on when $x$ is drawn from $P$.</p>
$$
\mathbb{E}_{x\sim P}[f(x)] = \sum P(x).f(x)
$$<p>for discrete random variable and</p>
$$
\mathbb{E}_{x\sim P}[f(x)] = \int P(x).f(x)dx
$$<p>Expectation are linear such that 
$$
\mathbb{E}_{x\sim P}[\alpha \cdot f(x) + \beta \cdot g(x)] = \alpha \mathbb{E}_{x\sim P}[f(x)] + \beta \mathbb{E}_{x\sim P}[g(x)]
$$</p>
<p>Variance is a measure of how much the value of a function of random variable $X$ vary as we sample different value of $x$ from its probability distribution.
$$
Var(f(x)) =\mathbb{E}([f(x)-\mathbb{E}[f(x)]^2])
$$
The square root of the variance is know as standard deviation. On the other hand the covarince give some sense of how much two value are linearly related to each other as well as the scale of these value.</p>
$$
Cov(f(x), g(y)) = \mathbb{E}[(f(x)- \mathbb{E}[f(x)])(g(y)- \mathbb{E}[g(y)])]
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Information-theory">
<a class="anchor" href="#Information-theory" aria-hidden="true"><span class="octicon octicon-link"></span></a>Information theory<a class="anchor-link" href="#Information-theory"> </a>
</h2>
<p>Information theory deals with quantification of how much information is present in a signal. In context of machine learning, information theory we apply information theory to: <em>characterize probability distributions</em> and <em>quantify similarities between probability distributions</em>. The following are the key information concepts and their application to machine learning.</p>
<h3 id="Entropy,-Cross-Entropy-and-Mutual-information">
<a class="anchor" href="#Entropy,-Cross-Entropy-and-Mutual-information" aria-hidden="true"><span class="octicon octicon-link"></span></a>Entropy, Cross Entropy and Mutual information<a class="anchor-link" href="#Entropy,-Cross-Entropy-and-Mutual-information"> </a>
</h3>
<p>Entropy give measure of uncertainty in a random experiment. It help us  quantify the amount of uncertainty in an entire probability distribution. The entropy of a probability distribution is the expected amount of information in an event drawn from that distribution defined as.</p>
$$
H(X) = -\mathbb{E}_{x \sim P}[\log P(x)] = -\sum_{i=1}^n P(x_i)l\log P(x_i)
$$<p>Entropy is widely used in model selection based on principle of maximum entropy. On the other hand, cross entropy is used to compare two probability distribution. It tell how similar two distribution are. The cross entropy between two probability distribution $P$ and $$Q$ defined over same set of outcome is given by</p>
$$
H(P,Q)= -\sum P(x)\log Q(x)
$$<p></p>
<p>Cross entropy loss function is widely used in machine learning for classification problem. The mutual information over two random variables help us gain insight about the information that one random variable carries about the other.</p>
$$
\begin{aligned}
I(X, Y) &amp;= \sum P(x, y)\log \frac{P(x,y)}{P(x).P(y)}\\
        &amp;=H(X)- H(X\mid Y) = H(Y) - H(Y\mid X)
\end{aligned}
$$<p>From above equation the mutual information  give insight about how far $X$ and $Y$ from being independent from each other. Mutual information can be used in feature selection instead of correlation as it capture both linear and non linear dependency.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Kullback-leibler-Divergence">
<a class="anchor" href="#Kullback-leibler-Divergence" aria-hidden="true"><span class="octicon octicon-link"></span></a>Kullback-leibler Divergence<a class="anchor-link" href="#Kullback-leibler-Divergence"> </a>
</h3>
<p><strong>Kullback-leibler Divergence</strong> measure how one probability distribution diverge from the other. Given two probability distribution $P(x)$ and $Q(X)$ where the former is the modelled/estimated distribution and the later is the actual/expected distribution. The <strong>KL</strong> divergence is defined as</p>
$$
\begin{aligned}
D_{KL}(P||Q) &amp; = \mathbb{E}_{x \sim P} [\log \frac{P(x)}{Q(x)}]\\
             &amp; = \mathbb{E}_{x \sim P}[\log P(x)] - \mathbb{E}_{x \sim P}[\log Q(x)]
\end{aligned}
$$<p>For discrete random distribution</p>
$$
D_{KL}(P||Q) = \sum_{i} P(x_i)\log \frac{P(x_i)}{Q(x_i)}
$$<p>And for continuous random variable</p>
$$
D_{KL}(p||q) = \int_{x} p(x) \log \frac{p(x)}{q(x)}
$$<p>KL divergence between $P$ and $Q$ tells how much information we lose when trying to approximate data given by $P$ with $Q$. It is non-negative $D_{KL}(P\mid \mid Q) \geq 0$ and  $0$ if $P$ and $Q$ are the same (distribution discrete) or equal almost anywhere in the case of continuous distribution. Apart from that KL divergence is not symmetric $D_{KL}(P\mid \mid Q) \neq D_{KL}(P\mid \mid Q)$ because of this it is not a true distance measure.</p>
<p><strong>Relation between KL divergence and Cross Entropy</strong></p>
$$
\begin{aligned}
D_{KL}(P||Q) &amp; = \mathbb{E}_{x \sim P} [\log \frac{P(x)}{Q(x)}]\\
             &amp; = \mathbb{E}_{x \sim P}[\log P(x)] - \mathbb{E}_{x \sim P}[\log Q(x)]\\
             &amp; = H(P) - H(P, Q)\\
\end{aligned}
$$<p>where $\mathbb{E}_{x \sim P}[\log P(x)] = H(P)$$ and $$\mathbb{E}_{x \sim P}[\log Q(x)] = H(P, Q)$. Thus  $H(P,Q) = H(P) - D_{KL}(P||Q)$. This implies that minimizing cross entropy with respect to $Q$ is equivalent to minimizing the KL divergence. KL divergence is used in unsupervised machine learning technique like variational auto-encoder. The KL divergence is also used  as objective function in variational bayesian method to find optimal value for approximating distribution.</p>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="sambaiga/sambaiga"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/sambaiga/jupyter/2018/02/01/probabilities.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer">

  <div class="wrapper">
<div class="wrapper">
     Copyright 2020 
    
    
    : sambaiga@gmail.com
  </div>

  </div>
</footer>



<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>
<script src="/assets/js/common.js"></script>

<script>
  $("script[type='math/tex']").replaceWith(function() {
      var tex = $(this).text();
      return katex.renderToString(tex.replace(/%.*/g, ''), {displayMode: false});
  });

  $("script[type='math/tex; mode=display']").replaceWith(function() {
      var tex = $(this).html();
      return katex.renderToString(tex.replace(/%.*/g, ''), {displayMode: true});
  });
</script>


</body>

</html>
