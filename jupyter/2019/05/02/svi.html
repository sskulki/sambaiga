<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Stochastic Variational Inference (SVI) | sambaiga</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Stochastic Variational Inference (SVI)" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This post introduce stochastic gradient based algorithm (SVI) used in practise to do variational inference under mean filed assumptions. It also present two important tricks namely re-parametrization trick and amortized inference that are useful when using SVI in solving problems." />
<meta property="og:description" content="This post introduce stochastic gradient based algorithm (SVI) used in practise to do variational inference under mean filed assumptions. It also present two important tricks namely re-parametrization trick and amortized inference that are useful when using SVI in solving problems." />
<link rel="canonical" href="https://sambaiga.github.io/sambaiga/jupyter/2019/05/02/svi.html" />
<meta property="og:url" content="https://sambaiga.github.io/sambaiga/jupyter/2019/05/02/svi.html" />
<meta property="og:site_name" content="sambaiga" />
<meta property="og:image" content="https://sambaiga.github.io/sambaiga/images/svi.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-05-02T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"This post introduce stochastic gradient based algorithm (SVI) used in practise to do variational inference under mean filed assumptions. It also present two important tricks namely re-parametrization trick and amortized inference that are useful when using SVI in solving problems.","headline":"Stochastic Variational Inference (SVI)","@type":"BlogPosting","dateModified":"2019-05-02T00:00:00-05:00","datePublished":"2019-05-02T00:00:00-05:00","image":"https://sambaiga.github.io/sambaiga/images/svi.jpg","mainEntityOfPage":{"@type":"WebPage","@id":"https://sambaiga.github.io/sambaiga/jupyter/2019/05/02/svi.html"},"url":"https://sambaiga.github.io/sambaiga/jupyter/2019/05/02/svi.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/sambaiga/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://sambaiga.github.io/sambaiga/feed.xml" title="sambaiga" /><link rel="shortcut icon" type="image/x-icon" href="/sambaiga/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Stochastic Variational Inference (SVI) | sambaiga</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Stochastic Variational Inference (SVI)" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This post introduce stochastic gradient based algorithm (SVI) used in practise to do variational inference under mean filed assumptions. It also present two important tricks namely re-parametrization trick and amortized inference that are useful when using SVI in solving problems." />
<meta property="og:description" content="This post introduce stochastic gradient based algorithm (SVI) used in practise to do variational inference under mean filed assumptions. It also present two important tricks namely re-parametrization trick and amortized inference that are useful when using SVI in solving problems." />
<link rel="canonical" href="https://sambaiga.github.io/sambaiga/jupyter/2019/05/02/svi.html" />
<meta property="og:url" content="https://sambaiga.github.io/sambaiga/jupyter/2019/05/02/svi.html" />
<meta property="og:site_name" content="sambaiga" />
<meta property="og:image" content="https://sambaiga.github.io/sambaiga/images/svi.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-05-02T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"This post introduce stochastic gradient based algorithm (SVI) used in practise to do variational inference under mean filed assumptions. It also present two important tricks namely re-parametrization trick and amortized inference that are useful when using SVI in solving problems.","headline":"Stochastic Variational Inference (SVI)","@type":"BlogPosting","dateModified":"2019-05-02T00:00:00-05:00","datePublished":"2019-05-02T00:00:00-05:00","image":"https://sambaiga.github.io/sambaiga/images/svi.jpg","mainEntityOfPage":{"@type":"WebPage","@id":"https://sambaiga.github.io/sambaiga/jupyter/2019/05/02/svi.html"},"url":"https://sambaiga.github.io/sambaiga/jupyter/2019/05/02/svi.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://sambaiga.github.io/sambaiga/feed.xml" title="sambaiga" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    // remove paragraph tags in rendered toc (happens from notebooks)
    var toctags = document.querySelectorAll(".toc-entry")
    toctags.forEach(e => (e.firstElementChild.innerText = e.firstElementChild.innerText.replace('¶', '')))
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/sambaiga/">sambaiga</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/sambaiga/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Stochastic Variational Inference (SVI)</h1><p class="page-description">This post introduce  stochastic gradient based algorithm (SVI) used in practise to do variational inference under mean filed assumptions. It also present two important tricks namely re-parametrization trick and amortized inference that are useful when using SVI in solving problems.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2019-05-02T00:00:00-05:00" itemprop="datePublished">
        May 2, 2019
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      8 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/sambaiga/categories/#jupyter">jupyter</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/sambaiga/sambaiga/tree/master/_notebooks/2019-05-02-svi.ipynb" role="button">
<img class="notebook-badge-image" src="/sambaiga/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div><div class="px-2">
    <a href="https://colab.research.google.com/github/sambaiga/sambaiga/blob/master/_notebooks/2019-05-02-svi.ipynb">
        <img class="notebook-badge-image" src="/sambaiga/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#Stochastic-Variational-Inference-(SVI)">Stochastic Variational Inference (SVI) </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Natural-Gradient-for-SVI">Natural Gradient for SVI </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Re-parametrization-trick">Re-parametrization trick </a></li>
<li class="toc-entry toc-h2"><a href="#Amortized-Variational-Inference">Amortized Variational Inference </a></li>
<li class="toc-entry toc-h2"><a href="#Reference">Reference </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2019-05-02-svi.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The <a href="">previous post</a>  introduced the basic principle of Variational Inference (VI) as one of the approach used to approximate difficult probability distribution, derived the ELBO function and discussed about Mean Field Variational Inference (MFVI) and the Coordinate Ascent Variational Inference (CAVI) algorithms. This post introduce another  stochastic gradient based algorithm (SVI) used in practise to do VI under mean filed assumptions. It also present two important tricks re-parametrization trick and amortized inference that are useful when using SVI in solving problems.</p>
<h2 id="Stochastic-Variational-Inference-(SVI)">
<a class="anchor" href="#Stochastic-Variational-Inference-(SVI)" aria-hidden="true"><span class="octicon octicon-link"></span></a>Stochastic Variational Inference (SVI)<a class="anchor-link" href="#Stochastic-Variational-Inference-(SVI)"> </a>
</h2>
<p>Consider the graphical model of the observations $\mathbf{x}$ and latent variable $\mathbf{z}=\{\theta, z\}$ in figure 1 where $\theta$ is the global variable and $z = \{z_1, \ldots z_n\}$  is the local (per-data-point) variable such that:</p>
<p><img src="/sambaiga/images/copied_from_nb/my_icons/VI.png" alt=""></p>
$$
p(\mathbf{x},\mathbf{z}) = p(\theta|\alpha)\prod_{i=1}^N p(x_i|z_i, \theta)\cdot p(z_i|\alpha)
$$<p></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Similarly the variational parameters are given by $\lambda = \{\gamma, \phi\} $ where the variational parameter $\gamma$ correspond to latent variable  and  $\phi$ denote set of local variational parameters. The variational distribution $q(\mathbf{z}\mid \phi)$ is given by</p>
$$
q(\mathbf{z}\mid \phi) = q(\theta|\gamma)\prod_{i=1}^N q(z_i|\phi_i, \alpha)
$$<p>which  also depend on hyper-parameter $\alpha$. The ELBO of this graphical model $\mathcal{L}_{VI}(q) = \mathbb{E}_q[ \log p(\mathbf{x},\mathbf{z}, \alpha) -\log q(\mathbf{z}, \gamma)]$ has the following form:</p>
$$
\begin{split}
\mathcal{L}_{VI}(q) &amp;= \mathbb{E}_q[ \log p(\theta|\alpha)- \log q(\theta|\gamma)] \\
&amp;+ \sum_{i=1}^{N}\mathbb{E}_q[\log p(z_i|\theta) 
+ \log p(x_i|z_i, \theta)-\log q(z_i|\phi_i)]
\end{split}
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The equation above could be optimized by CAVI algorithm discussed in previous post which is expensive for large data sets. The CAVI algorithm scales with $N$ as it require to optimize the local variational parameters for each data point before re-estimating the global variational parameters.</p>
<p>Unlike CAI, SVI uses stochastic optimization to fit the global variational parameters by repeatedly sub-sample the data to form stochastic estimate of ELBO. In every iteration one randomly selects mini-batches of size $b_{sz}$  to obtain a stochastic estimate of ELBO.</p>
$$
\begin{split}
\mathcal{L}_{VI}(q) &amp;= \mathbb{E}_q[ \log p(\theta|\alpha)- \log q(\theta|\gamma)] \\
&amp;+ \frac{N}{b_{sz}}\sum_{s=1}^{b_{sz}}\mathbb{E}_q[\log p(z_{i_s}|\theta) + \log p(x_{i_s}|z_{i_s}, \theta)-\log q(z_{i_s}|\phi_{i_s})]
\end{split}
$$<p>SVI algorithms follow noisy estimates of the gradient with a decreasing step size which is often  cheaper to compute than the true gradient. Following such noisy estimates allows SVI to escape  shallow local optima of complex objective functions.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Natural-Gradient-for-SVI">
<a class="anchor" href="#Natural-Gradient-for-SVI" aria-hidden="true"><span class="octicon octicon-link"></span></a>Natural Gradient for SVI<a class="anchor-link" href="#Natural-Gradient-for-SVI"> </a>
</h3>
<p>To solve the optimization problem standard gradient-based methods such as SGD, Adam or Adagrad can be used. However, for SVI these gradient based methods cause slow convergence or converge to inferior local models. This is because, gradient based methods use the following update</p>
$$
\theta^{t+1}=\theta^t + \alpha \frac{\partial \mathcal{L}_{VI}(q)}{\partial \theta} 
$$<p>where</p>
$$
\frac{\partial \mathcal{L}_{VI}(q)}{\partial \theta} =\frac{\partial \mathcal{L}_{VI}(q)}{\partial \theta_1}, \ldots \frac{\partial \mathcal{L}_{VI}(q)}{\partial \theta_k}
$$<p></p>
<p>is the the gradient vector which point in the direction where the function increases most quickly while the changes in the function are measured with respect to euclidean distance. As the result, if the euclidean distance between the variational parameter being optimized is not good measure of variation in objective function then gradient descent will move suboptimal through the parameter value.</p>
<p>Consider the following  two set of gausian distributions $$\{d_{(1)}=\mathcal{N}(-2, 3), d_{(2)}=\mathcal{N}(2, 3)\}$$ and $$\{d_{(1)}=\mathcal{N}(-2, 1), d_{(2)}=\mathcal{N}(2, 1)\}$$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The euclidean distance between the two distributions $d_{}=\sqrt{(\mu_1-\mu_2)^2+ (\sigma^2_1-\sigma^2_2)^2}=4$ It clear that, considering only the euclidean distance the two images are the same. However, when we consider the shape of the distribution, the distance is different in the first and second image. In the first image, the KL-divergence should be lower as there is more overlap between between the two distribution unlike the second image where their support barely overlap. The reason for this difference is that probability distribution do not naturally fit in euclidean space rather it fit on a statistical manifold also called <a href="https://en.wikipedia.org/wiki/Riemannian_manifold">Riemannian manifold</a>.</p>
<p>Statistical manifold give a natural way of measuring distances between distribution that euclidean distance use in SGD. A common Riemannian metric for statistical manifold is the <a href="https://wiseodd.github.io/techblog/2018/03/11/fisher-information/">fisher information matrix</a> defined by</p>
$$
F_{\lambda} = \mathbb{E}_{p(x;\lambda)}[\nabla \log p(x;\lambda) (\nabla \log p(x;\theta))^T ]
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It can be showed that the fisher information matrix $F_{\lambda}$ is  the second derivative of the KL divergence between two distributions.</p>
$$
F_{\theta} = \nabla^2_{\theta} KL(q(x;\lambda)||p(x;\theta))
$$<p>Thus for SVI, the standard gradients descent techniques can be replaced  with the natural gradient as follows:</p>
$$
        \tilde{\nabla_{q}} \mathcal{L}(q) = F^{-1} \nabla{q}\mathcal{L}_{VI}(q)
$$<p>The update procedure for natural gradient can be summarized as follows:</p>
<ol>
<li>Compute the loss $\mathcal{L}_{VI}(q)$</li>
<li>Compute the gradient of the loss $\nabla{q}\mathcal{L}_{VI}(q)$</li>
<li>Compute the Fisher Information Matrix F.</li>
<li>Compute the natural gradient $\tilde{\nabla_{q}} \mathcal{L}_{VI}(q)$</li>
<li>Update the parameter $q^{t+1} =q^t - \alpha \tilde{\nabla_{\theta}}\mathcal{L}_{VI}(q)$</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Using natural gradient instead of standard gradients simplify SVI gradient update. However the same conditions for convergence as standard SDG have to be fulfilled. First, the mini-batch indices must be drawn uniformly at random size where the size $b_{sz}$ of the mini-batch must satisfies $1\leq b_{sz} \leq N$ The learning rate $\alpha$ needs to decrease with iterations $t$ satisying the <a href="https://en.wikipedia.org/wiki/Stochastic_approximation">Robbins Monro conditions</a> $\sum_{t=1}^{\infty} \alpha_t =\infty$ and $\sum_{t=1}^{\infty} \alpha_t^2 &lt;\infty$ This guarantee that every point in the parameter space can be reached while the gradient noise decreases quickly enough to ensure convergence.</p>
<p>The next section presents two important tricks namely re-parametrization trick and amortized inference that are useful when using SVI in solving problems.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Re-parametrization-trick">
<a class="anchor" href="#Re-parametrization-trick" aria-hidden="true"><span class="octicon octicon-link"></span></a>Re-parametrization trick<a class="anchor-link" href="#Re-parametrization-trick"> </a>
</h2>
<p>Consider the graphical model presented in figure 1, where gradient based stochastic optimization is used to learn the variational parameter $\phi$. For example; for Gaussian distribution  $q_{\phi}(z|x)=\mathcal{N}(\mu_{\phi}(x), \Sigma_{\phi}(x))$</p>
<p>To maximize the likelihood of the data, we need to back propagate the loss to the parameter $\phi$ across the distribution of $z$ or across sample $z\sim q_\phi(z \mid x) $ However, it is difficulty to back-propagate through random variable. To address this problem, the re-parametrization trick is used.</p>
<p>First let consider the <a href="https://en.wikipedia.org/wiki/Law_of_the_unconscious_statistician">Law of the Unconscious Statistician (LOTUS)</a>, that is used to calculate the expected value of a function $g(\epsilon)$ of a random variable $\epsilon$ when only the probability distribution $p(\epsilon)$ of $\epsilon$ is known. The Law state that:</p>
<blockquote>
<p>To compute the expectation of a measurable function $g(.)$ of a random variable $\epsilon$, we have to integrate $g(\epsilon)$ with respect to the distribution function of $\epsilon$, that is: $$\mathbb{E}(g(\epsilon)) = \int g(\epsilon)dF_{\epsilon}(\epsilon)
$$</p>
</blockquote>
<p>In other words, to compute the expectation of $z =g(\epsilon)$ we only need to know $g(.)$ and the distribution of $\epsilon$. We do not need to explicitly know the distribution of $z$. Thus the above equation can be expression in the convenient alternative notation:</p>
$$
\mathbb{E}_{\epsilon \sim p(\epsilon)}(g(\epsilon)) = \mathbb{E}_{z \sim p(z)} (z)
$$<p>Therefore the reparameteriztaion trick states that:</p>
<blockquote>
<p>A random variable $z$ with distribution $q_{\phi}(z, \phi)$ which is independent to $\phi$ can be expressed as transformation of random variable $\epsilon \sim p(\epsilon)$ that come from noise distribution such as uniform or gaussian such that $z = g(\phi, \epsilon)$</p>
</blockquote>
<p>For instance for Gaussian variable $z$ in the above example</p>
$$
z = \mu(\phi) + \sigma^2(\phi)\cdot \epsilon
$$<p>where $\epsilon \sim \mathcal{N}(0, 1)$. Since $p(\epsilon)$ is independent of the parameter of $q_{\phi}(z, \phi)$, we can apply the change of variables in integral theory to compute any expectation over $z$ or any expectation over  $\phi$. The SDG estimator can therefore be estimated by pulling the gradient into expectations and approximating it by samples from the noise 
distribution such that  for any measurable function $f_{\theta}(.)$: $$\Delta_{\phi}\mathbb{E}_{z\sim p_{\phi}(z)} = \frac{1}{M}\sum_{i=1}^M \Delta f(g(\phi, \epsilon_i)) 
$$</p>
<p>where $\epsilon_i\sim p(\epsilon)$ , $f_{\theta}(.)$ must be differentiable w.r.t  its input $z$ and $g(\phi, \epsilon_i)$ must exist and be differentiable with respect to $\phi$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Amortized-Variational-Inference">
<a class="anchor" href="#Amortized-Variational-Inference" aria-hidden="true"><span class="octicon octicon-link"></span></a>Amortized Variational Inference<a class="anchor-link" href="#Amortized-Variational-Inference"> </a>
</h2>
<p>Consider the graphical model presented in figure 1 where ecah data point $x_i$ is governed by its latent variable $z_i$ with variational parameter $phi_i$ such that</p>
$$
q(\mathbf{z}\mid \phi) = q(\theta|\gamma)\prod_{i=1}^N q(z_i|\phi_i, \alpha)
$$<p>Using traditional SVI make it necessary to optimize $\phi_i$ for each data point $x_i$. As the results the number parameters to be optimized will grows with the number of observations $x$. This is not ideal for larger datasets. Apart from that, it requires one to re-run the optimization procedure in case of new observation or when we have to perform inference. To address these problem amortized VI introduce a parametrized function that maps from observation space to the parameter of the approximate posterior distribution.</p>
<p>Amortized VI try to learn from past inference/pre-computation so that future inferences run faster. Instead of approximating separate variables for each data point $x_i$, amortized VI assume that the local variational parameter $\phi$ can be predicted by a parametrized function $f_{\phi}(.)$ of data whose parameters are shared across all data points. Thus instead of introducing local variational parameter, we learn a single parametric function and work with a variational distribution that has the form</p>
$$
q(\mathbf{z}\mid \phi) = q(\theta|\gamma)\prod_{i=1}^N q(z_i|f_{\phi}(.))
$$<p>where $f_{\phi}(.)$ is the deep neural net function of $z$</p>
<p>Deep neural network used in this context are called <a href="">inference networks</a>. Therefore amortized inference with inference networks combines probabilistic modelling with representation power of deep learning. Using amortized VI instead of traditional VI, has two important advantages. First the number of variational parameters remain constant with respect to the data size. We only need to specify the parameter of the neural networks which is independent to the number of observations. Second, for new observation or during inference all we need to do is to call the inference network. As the result, we can invest time upfront optimizing the inference network and during inference we use the trained network for fast inference.</p>
<h2 id="Reference">
<a class="anchor" href="#Reference" aria-hidden="true"><span class="octicon octicon-link"></span></a>Reference<a class="anchor-link" href="#Reference"> </a>
</h2>
<ol>
<li>
<a href="https://arxiv.org/abs/1711.05597">[Cheng Zhang,(2017)]</a>:
Advances in Variational Inference.</li>
<li>
<a href="https://arxiv.org/abs/1610.05735">[Daniel Ritchie,(2016)]</a>:Deep Amortized Inference for Probabilistic Programs.</li>
<li>
<a href="https://arxiv.org/abs/1610.05735">[Andrew Miller,(2016)]</a>:Natural Gradients and Stochastic Variational Inference.</li>
<li>
<a href="https://www.shakirm.com/papers/VITutorial.pdf">Shakir Mohamed</a>:Variational Inference  for Machine Learning. </li>
<li>
<a href="https://emtiyaz.github.io/teaching/ds3_2018/ds3.html">DS3 workshop</a>:Approximate Bayesian Inference: Old and New.</li>
<li>
<a href="https://github.com/philschulz/VITutorial">Variational Inference and Deep Generative Models</a>:Variational Inference for NLP audiences</li>
</ol>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="sambaiga/sambaiga"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/sambaiga/jupyter/2019/05/02/svi.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/sambaiga/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/sambaiga/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/sambaiga/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p></p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/sambaiga" title="sambaiga"><svg class="svg-icon grey"><use xlink:href="/sambaiga/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/sambaiga" title="sambaiga"><svg class="svg-icon grey"><use xlink:href="/sambaiga/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
